{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK (Natural Language Toolkit) Tutorial in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Natural Language Processing?\n",
    "Natural Language Processing is manipulation or understanding text or speech by any software or machine. An analogy is that humans interact, understand each other views, and respond with the appropriate answer. In NLP, this interaction, understanding, the response is made by a computer instead of a human."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLTK?\n",
    "NLTK stands for Natural Language Toolkit. This toolkit is one of the most powerful NLP libraries which contains packages to make machines understand human language and reply to it with an appropriate response. Tokenization, Stemming, Lemmatization, Punctuation, Character count, word count are some of these packages which will be discussed in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is what we cover in the tutorial\n",
    "\n",
    "* NLP (Natural Language Processing) Tutorial: What is, History, Example\n",
    "* Tokenize Words and Sentences with NLTK\n",
    "* POS (Part-Of-Speech) Tagging & Chunking with NLTK\n",
    "* Stemming and Lemmatization with Python NLTK\n",
    "* WordNet with NLTK: Finding Synonyms for words in Python\n",
    "* Counting POS Tags, Frequency Distribution & Collocations in NLTK\n",
    "* Word Embedding Tutorial: word2vec using Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP (Natural Language Processing) Tutorial: What is, History, Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Natural Language Processing?\n",
    "\n",
    "Natural Language Processing (NLP) is a branch of AI that helps computers to understand, interpret and manipulate human language.\n",
    "\n",
    "NLP helps developers to organize and structure knowledge to perform tasks like translation, summarization, named entity recognition, relationship extraction, speech recognition, topic segmentation, etc.\n",
    "\n",
    "NLP is a way of computers to analyze, understand and derive meaning from a human languages such as English, Spanish, Hindi, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of NLP\n",
    "Here, is are important events in the history of Natural Language Processing:\n",
    "\n",
    "1950- NLP started when Alan Turing published an article called \"Machine and Intelligence.\"\n",
    "\n",
    "1950- Attempts to automate translation between Russian and English\n",
    "\n",
    "1960- The work of Chomsky and others on formal language theory and generative syntax\n",
    "\n",
    "1990- Probabilistic and data-driven models had become quite standard\n",
    "\n",
    "2000- A Large amount of spoken and textual data become available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does NLP work?\n",
    "\n",
    "Before we learn how NLP works, let's understand how humans use language-\n",
    "\n",
    "Every day, we say thousand of a word that other people interpret to do countless things. We, consider it as a simple communication, but we all know that words run much deeper than that. There is always some context that we derive from what we say and how we say it., NLP never focuses on voice modulation; it does draw on contextual patterns.\n",
    "\n",
    "Example:\n",
    "\n",
    ">Man is to woman as king is to __________?\n",
    "\n",
    ">Meaning (king) – meaning (man) + meaning ( woman)=?\n",
    "\n",
    ">The answer is-  queen\n",
    "\n",
    "Here, we can easily co-relate because man is male gender and woman is female gender. In the same way, the king is masculine gender, and its female gender is queen.\n",
    "\n",
    "Example:\n",
    "\n",
    ">Is King to kings as the queen is to_______?\n",
    "\n",
    ">The answer is--- queens \n",
    "\n",
    "Here, we can see two words kings and kings where one is singular and other is plural. Therefore, when the world queen comes, it automatically co-relates with queens again singular plural.\n",
    "\n",
    "Here, the biggest question is that how do we know what words mean? Let's, say who will call it queen?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer is we learn this thinks through experience. However, here the main question is that how computer know about the same?\n",
    "\n",
    "We need to provide enough data for Machines to learn through experience. We can feed details like\n",
    "\n",
    "Her Majesty the Queen.\n",
    "The Queen's speech during the State visit\n",
    "The crown of Queen Elizabeth\n",
    "The Queens's Mother\n",
    "The queen is generous.\n",
    "With above examples the machine understands the entity Queen.\n",
    "\n",
    "The machine creates word vectors as below. A word vector is built using surrounding words.\n",
    "\n",
    "![](122118_0534_NLPNaturalL2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine creates these vectors\n",
    "\n",
    "As it learns from multiple datasets\n",
    "Use Machine learning (e.g., Deep Learning algorithms)\n",
    "A word vector is built using surrounding words.\n",
    "Here is the formula:\n",
    "\n",
    "Meaning (king) – meaning (man) + meaning (woman)=?\n",
    "\n",
    "This amounts to performing simple algebraic operations on word vectors:\n",
    "\n",
    "Vector ( king) – vector (man) + vector (woman)= vector(?)\n",
    "\n",
    "To which the machine answers queen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components of NLP\n",
    "\n",
    "Five main Component of Natural Language processing are:\n",
    "\n",
    "* Morphological and Lexical Analysis\n",
    "* Syntactic Analysis\n",
    "* Semantic Analysis\n",
    "* Discourse Integration\n",
    "* Pragmatic Analysis\n",
    "\n",
    "![](122118_0534_NLPNaturalL3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Morphological and Lexical Analysis**\n",
    "\n",
    "Lexical analysis is a vocabulary that includes its words and expressions. It depicts analyzing, identifying and description of the structure of words. It includes dividing a text into paragraphs, words and the sentences\n",
    "\n",
    "Individual words are analyzed into their components, and nonword tokens such as punctuations are separated from the words.\n",
    "\n",
    "**Syntax analysis**\n",
    "\n",
    "The words are commonly accepted as being the smallest units of syntax. The syntax refers to the principles and rules that govern the sentence structure of any individual languages.\n",
    "\n",
    "Syntax focus about the proper ordering of words which can affect its meaning. This involves analysis of the words in a sentence by following the grammatical structure of the sentence. The words are transformed into the structure to show hows the word are related to each other.\n",
    "\n",
    "**Discourse Integration**\n",
    "\n",
    "It means a sense of the context. The meaning of any single sentence which depends upon that sentences. It also considers the meaning of the following sentence.\n",
    "\n",
    "For example, the word \"that\" in the sentence \"He wanted that\" depends upon the prior discourse context.\n",
    "\n",
    "**Semantic Analysis**\n",
    "\n",
    "Semantic Analysis is a structure created by the syntactic analyzer which assigns meanings. This component transfers linear sequences of words into structures. It shows how the words are associated with each other.\n",
    "\n",
    "Semantics focuses only on the literal meaning of words, phrases, and sentences. This only abstracts the dictionary meaning or the real meaning from the given context. The structures assigned by the syntactic analyzer always have assigned meaning\n",
    "\n",
    "E.g.. \"colorless green idea.\" This would be rejected by the Symantec analysis as colorless Here; green doesn't make any sense.\n",
    "\n",
    "**Pragmatic Analysis**\n",
    "\n",
    "Pragmatic Analysis deals with the overall communicative and social content and its effect on interpretation. It means abstracting or deriving the meaningful use of language in situations. In this analysis, the main focus always on what was said in reinterpreted on what is meant.\n",
    "\n",
    "Pragmatic analysis helps users to discover this intended effect by applying a set of rules that characterize cooperative dialogues.\n",
    "\n",
    "E.g., \"close the window?\" should be interpreted as a request instead of an order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP and writing systems\n",
    "\n",
    "The kind of writing system used for a language is one of the deciding factors in determining the best approach for text pre-processing. Writing systems can be\n",
    "\n",
    "1. Logographic: a Large number of individual symbols represent words. Example Japanese, Mandarin\n",
    "2. Syllabic: Individual symbols represent syllables\n",
    "3. Alphabetic: Individual symbols represent sound\n",
    "\n",
    "Majority of the writing systems use the Syllabic or Alphabetic system. Even English, with its relatively simple writing system based on the Roman alphabet, utilizes logographic symbols which include Arabic numerals, Currency symbols (S, £), and other special symbols.\n",
    "\n",
    "This pose following challenges\n",
    "\n",
    "* Extracting meaning(semantics) from a text is a challenge\n",
    "* NLP is dependent on the quality of the corpus. If the domain is vast, it's difficult to understand context.\n",
    "* There is a dependence on the character set and language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to implement NLP\n",
    "\n",
    "Below, given are popular methods used for Natural Learning Process:\n",
    "\n",
    "**Machine learning**: The learning nlp procedures used during machine learning. It automatically focuses on the most common cases. So when we write rules by hand, it is often not correct at all concerned about human errors.\n",
    "\n",
    "**Statistical inference**: NLP can make use of statistical inference algorithms. It helps you to produce models that are robust. e.g., containing words or structures which are known to everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Examples\n",
    "\n",
    "Today, Natual process learning technology is widely used technology.\n",
    "\n",
    "Here, are common Application' of NLP:\n",
    "\n",
    "#### Information retrieval & Web Search\n",
    "\n",
    "Google, Yahoo, Bing, and other search engines base their machine translation technology on NLP deep learning models. It allows algorithms to read text on a webpage, interpret its meaning and translate it to another language.\n",
    "\n",
    "#### Grammar Correction:\n",
    "\n",
    "NLP technique is widely used by word processor software like MS-word for spelling correction & grammar check.\n",
    "\n",
    "#### Question Answering\n",
    "\n",
    "Type in keywords to ask Questions in Natural Language.\n",
    "\n",
    "#### Text Summarization\n",
    "\n",
    "The process of summarising important information from a source to produce a shortened version\n",
    "\n",
    "#### Machine Translation\n",
    "\n",
    "Use of computer applications to translate text or speech from one natural language to another.\n",
    "\n",
    "#### Sentiment analysis\n",
    "\n",
    "NLP helps companies to analyze a large number of reviews on a product. It also allows their customers to give a review of the particular product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future of NLP\n",
    "* Human readable natural language processing is the biggest Al- problem. It is all most same as solving the central artificial intelligence problem and making computers as intelligent as people.\n",
    "* Future computers or machines with the help of NLP will able to learn from the information online and apply that in the real world, however, lots of work need to on this regard.\n",
    "* Naturla language toolkit or nltk become more effective\n",
    "* Combined with natural language generation, computers will become more capable of receiving and giving useful and resourceful information or data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "* Natural Language Processing is a branch of AI which helps computers to understand, interpret and manipulate human language\n",
    "* NLP started when Alan Turing published an article called \"Machine and Intelligence\".\n",
    "* NLP never focuses on voice modulation; it does draw on contextual patterns\n",
    "* Five essential components of Natural Language processing are 1) Morphological and Lexical Analysis 2)Syntactic Analysis 3) Semantic Analysis 4) Discourse Integration 5) Pragmatic Analysis\n",
    "Three types of the Natural process writing system are 1)Logographic 2) Syllabic 3) Alphabetic\n",
    "* Machine learning and Statistical inference are two methods to implementation of Natural Process Learning\n",
    "* Essential Applications of NLP are Information retrieval & Web Search, Grammar Correction Question Answering, Text Summarization, Machine Translation, etc.\n",
    "* Future computers or machines with the help of NLP and Data Science will able to learn from the information online and apply that in the real world, however, lots of work need to on this regard\n",
    "* NLP is are ambiguous while open source computer language is designed to unambiguous\n",
    "The biggest advantage of the NLP system is that it offers exact answers to the questions, no unnecessary or unwanted information\n",
    "* The biggest draw back of the NLP system is built for a single and specific task only so it is unable to adapt to new domains and problems because of limited functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Words and Sentences with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Tokenization?\n",
    "Tokenization is the process by which big quantity of text is divided into smaller parts called **tokens**.\n",
    "\n",
    "Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc. It becomes vital to understand the pattern in the text to achieve the above-stated purpose. These tokens are very useful for finding such patterns as well as is considered as a base step for stemming and lemmatization.\n",
    "\n",
    "For the time being, don't worry about stemming and lemmatization but treat them as steps for textual data cleaning using NLP (Natural language processing). We will discuss stemming and lemmatization later in the tutorial. Tasks such as Text classification or spam filtering makes use of NLP along with deep learning libraries such as Keras and Tensorflow.\n",
    "\n",
    "Natural Language toolkit has very important module tokenize which further comprises of sub-modules\n",
    "\n",
    "* word tokenize\n",
    "* sentence tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of words\n",
    "\n",
    "We use the method word_tokenize() to split a sentence into words. The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications. It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming. Machine learning models need numeric data to be trained and make a prediction. Word tokenization becomes a crucial part of the text (string) to numeric data conversion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['God', 'is', 'Great', '!', 'I', 'won', 'a', 'lottery', '.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "* word_tokenize module is imported from the NLTK library.\n",
    "* A variable \"text\" is initialized with two sentences.\n",
    "* Text variable is passed in word_tokenize module and printed the result. This module breaks each word with punctuation which you can see in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization of Sentences\n",
    "Sub-module available for the above is sent_tokenize. An obvious question in your mind would be why sentence tokenization is needed when we have the option of word tokenization. Imagine you need to count average words per sentence, how you will calculate? For accomplishing such a task, you need both sentence tokenization as well as words to calculate the ratio. Such output serves as an important feature for machine training as the answer would be numeric.\n",
    "\n",
    "Check the below example to learn how sentence tokenization is different from words tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['God is Great!', 'I won a lottery.']\n"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the program**:\n",
    "\n",
    "* In a line like the previous program, imported the sent_tokenize module.\n",
    "* We have taken the same sentence. Further sent module parsed that sentences and show output. It is clear that this function breaks each sentence.\n",
    "\n",
    "Above examples are good settings stones to understand the mechanics of the word and sentence tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS (Part-Of-Speech) Tagging & Chunking with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "Parts of speech Tagging is responsible for reading the text in a language and assigning some specific token (Parts of Speech) to each word.\n",
    "\n",
    "e.g.\n",
    "\n",
    "Input: Everything to permit us.\n",
    "\n",
    "Output: [('Everything', NN),('to', TO), ('permit', VB), ('us', PRP)]\n",
    "\n",
    "**Steps Involved**:\n",
    "\n",
    "* Tokenize text (word_tokenize)\n",
    "* apply pos_tag to above step that is nltk.pos_tag(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviation |Meaning\n",
    "-------------|:------------\n",
    "CC |\tcoordinating conjunction\n",
    "CD |\tcardinal digit\n",
    "DT |\tdeterminer\n",
    "EX |\texistential there\n",
    "FW |\tforeign word\n",
    "IN |\tpreposition/subordinating conjunction\n",
    "JJ |\tadjective (large)\n",
    "JJR |\tadjective, comparative (larger)\n",
    "JJS |\tadjective, superlative (largest)\n",
    "LS |\tlist market\n",
    "MD |\tmodal (could, will)\n",
    "NN |\tnoun, singular (cat, tree)\n",
    "NNS |\tnoun plural (desks)\n",
    "NNP |\tproper noun, singular (sarah)\n",
    "NNPS |\tproper noun, plural (indians or americans)\n",
    "PDT |\tpredeterminer (all, both, half)\n",
    "POS |\tpossessive ending (parent\\ 's)\n",
    "PRP |\tpersonal pronoun (hers, herself, him,himself)\n",
    "PRP$ |\tpossessive pronoun (her, his, mine, my, our )\n",
    "RB |\tadverb (occasionally, swiftly)\n",
    "RBR |\tadverb, comparative (greater)\n",
    "RBS |\tadverb, superlative (biggest)\n",
    "RP |\tparticle (about)\n",
    "TO |\tinfinite marker (to)\n",
    "UH |\tinterjection (goodbye)\n",
    "VB |\tverb (ask)\n",
    "VBG |\tverb gerund (judging)\n",
    "VBD |\tverb past tense (pleaded)\n",
    "VBN |\tverb past participle (reunified)\n",
    "VBP |\tverb, present tense not 3rd person singular(wrap)\n",
    "VBZ |\tverb, present tense with 3rd person singular (bases)\n",
    "WDT |\twh-determiner (that, what)\n",
    "WP |\twh- pronoun (who)\n",
    "WRB |\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Chunking is used to add more structure to the sentence by following parts of speech (POS) tagging. It is also known as shallow parsing. The resulted group of words is called \"chunks.\" In shallow parsing, there is maximum one level between roots and leaves while deep parsing comprises of more than one level. Shallow Parsing is also called light parsing or chunking.\n",
    "\n",
    "The primary usage of chunking is to make a group of \"noun phrases.\" The parts of speech are combined with regular expressions.\n",
    "\n",
    "**Rules for Chunking**:\n",
    "\n",
    "There are no pre-defined rules, but you can combine them according to need and requirement.\n",
    "\n",
    "For example, you need to tag Noun, verb (past tense), adjective, and coordinating junction from the sentence. You can use the rule as below\n",
    "\n",
    "`chunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "After Split: ['learn', 'php', 'from', 'guru99', 'and', 'make', 'study', 'easy']\nAfter Token: [('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN'), ('and', 'CC'), ('make', 'VB'), ('study', 'NN'), ('easy', 'JJ')]\nAfter Regex: chunk.RegexpParser with 1 stages:\nRegexpChunkParser with 1 rules:\n       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\nAfter Chunking (S\n  (mychunk learn/JJ)\n  (mychunk php/NN)\n  from/IN\n  (mychunk guru99/NN and/CC)\n  make/VB\n  (mychunk study/NN easy/JJ))\n"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"learn php from guru99 and make study easy\".split()\n",
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case of Chunking\n",
    "\n",
    "Chunking is used for entity detection. An entity is that part of the sentence by which machine get the value for any intention\n",
    "\n",
    "Example: \n",
    "`Temperature of New York. `\n",
    "\n",
    "Here Temperature is the intention and New York is an entity. \n",
    "\n",
    "In other words, chunking is used as selecting the subsets of tokens. Please follow the below code to understand how chunking is used to select the tokens. In this example, you will see the graph which will correspond to a chunk of a noun phrase. We will write the code and draw the graph for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['learn', 'php', 'from', 'guru99']\n[('learn', 'JJ'), ('php', 'NN'), ('from', 'IN'), ('guru99', 'NN')]\n(S (NP learn/JJ php/NN) from/IN (NP guru99/NN))\n"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"learn php from guru99\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "tag = nltk.pos_tag(tokens)\n",
    "print(tag)\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp  =nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tag)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is used to categorize different tokens into the same chunk. The result will depend on grammar which has been selected. Further chunking is used to tag patterns and to explore text corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization with Python NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Stemming?\n",
    "Stemming is a kind of normalization for words. Normalization is a technique where a set of words in a sentence are converted into a sequence to shorten its lookup. The words which have the same meaning but have some variation according to the context or sentence are normalized.\n",
    "\n",
    "In another word, there is one root word, but there are many variations of the same words. For example, the root word is \"eat\" and it's variations are \"eats, eating, eaten and like so\". In the same way, with the help of Stemming, we can find the root word of any variations.\n",
    "\n",
    "**For example**\n",
    "\n",
    ">He was riding.\t\n",
    "\n",
    ">He was taking the ride.\n",
    "\n",
    "In the above two sentences, the meaning is the same, i.e., riding activity in the past. A human can easily understand that both meanings are the same. But for machines, both sentences are different. Thus it became hard to convert it into the same data row. In case we do not provide the same data-set, then machine fails to predict. So it is necessary to differentiate the meaning of each word to prepare the dataset for machine learning. And here stemming is used to categorize the same type of data by getting its root word.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this with a Python program.NLTK has an algorithm named as \"PorterStemmer\". This algorithm accepts the list of tokenized word and stems it into root word.\n",
    "\n",
    "**Program for understanding Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "wait\nwait\nwait\nwait\n"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**:\n",
    "\n",
    "* There is a stem module in NLTk which is imported. If ifyou import the complete module, then the program becomes heavy as it contains thousands of lines of codes. So from the entire stem module, we only imported \"PorterStemmer.\"\n",
    "*  We prepared a dummy list of variation data of the same word.\n",
    "*  An object is created which belongs to class nltk.stem.porter.PorterStemmer.\n",
    "*  Further, we passed it to PorterStemmer one by one using \"for\" loop. Finally, we got output root word of each word mentioned in the list.\n",
    "\n",
    "From the above explanation, it can also be concluded that stemming is considered as an important preprocessing step because it removed redundancy in the data and variations in the same word. As a result, data is filtered which will help in better machine training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "hello\nguru99\n,\nyou\nhave\nto\nbuild\na\nveri\ngood\nsite\nand\nI\nlove\nvisit\nyour\nsite\n.\n"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence=\"Hello Guru99, You have to build a very good site and I love visiting your site.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "\trootWord=ps.stem(w)\n",
    "\tprint(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "* Package PorterStemer is imported from module stem\n",
    "* Packages for tokenization of sentence as well as words are imported\n",
    "* A sentence is written which is to be tokenized in the next step.\n",
    "* Word tokenization is implemented in this step.\n",
    "* An object for PorterStemmer is created here.\n",
    "* Loop is run and stemming of each word is done using the object created in the code line 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "\n",
    "Stemming is a data-preprocessing module. The English language has many variations of a single word. These variations create ambiguity in machine learning training and prediction. To create a successful model, it's vital to filter such words and convert to the same type of sequenced data using stemming. Also, this is an important technique to get row data from a set of sentence and removal of redundant data also known as normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Lemmatization?\n",
    "\n",
    "Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. Lemmatization usually refers to the morphological analysis of words, which aims to remove inflectional endings. It helps in returning the base or dictionary form of a word, which is known as the lemma. The NLTK Lemmatization method is based on WorldNet's built-in morph function. Text preprocessing includes both stemming as well as lemmatization. Many people find the two terms confusing. Some treat these as same, but there is a difference between these both. Lemmatization is preferred over the former because of the below reason.\n",
    "\n",
    "#### Why is Lemmatization better than Stemming?\n",
    "\n",
    "Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.\n",
    "\n",
    "On the contrary, Lemmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms. In-depth linguistic knowledge is required to create dictionaries and look for the proper form of the word. Stemming is a general operation while lemmatization is an intelligent operation where the proper form will be looked in the dictionary. Hence, lemmatization helps in forming better machine learning features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to distinguish between Lemmatization and Stemming\n",
    "**Stemming code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Stemming for studies is studi\nStemming for studying is studi\nStemming for cries is cri\nStemming for cry is cri\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "  print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Lemma for studies is study\nLemma for studying is studying\nLemma for cries is cry\nLemma for cry is cry\n"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion of output:\n",
    "If you look stemming for studies and studying, output is same (studi) but lemmatizer provides different lemma for both tokens study for studies and studying for studying. So when we need to make feature set to train machine, it would be great if lemmatization is preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case of Lemmatizer:\n",
    "Lemmatizer minimizes text ambiguity. Example words like bicycle or bicycles are converted to base word bicycle. Basically, it will convert all words having the same meaning but different representation to their base form. It reduces the word density in the given text and helps in preparing the accurate features for training machine. Cleaner the data, the more intelligent and accurate your machine learning model, will be. Lemmatizerwill also saves memory as well as computational cost.\n",
    "\n",
    "Real Time example showing use of Wordnet Lemmatization and POS Tagging in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "guru99 NN => guru99\nis VBZ => be\na DT => a\ntotally RB => totally\nnew JJ => new\nkind NN => kind\nof IN => of\nlearning VBG => learn\nexperience NN => experience\n. . => .\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text = \"guru99 is a totally new kind of learning experience.\"\n",
    "tokens = word_tokenize(text)\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
    "    print(token, tag,  \"=>\", lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "guru99 NN => guru99\nis VBZ => is\na DT => a\ntotally RB => totally\nnew JJ => new\nkind NN => kind\nof IN => of\nlearning VBG => learning\nexperience NN => experience\n. . => .\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text = \"guru99 is a totally new kind of learning experience.\"\n",
    "tokens = word_tokenize(text)\n",
    "lemma_function = WordNetLemmatizer()\n",
    "for token, tag in pos_tag(tokens):\n",
    "    lemma = lemma_function.lemmatize(token)\n",
    "    print(token, tag,  \"=>\", lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "* Firstly, the corpus reader wordnet is imported.\n",
    "* WordNetLemmatizer is imported from wordnet\n",
    "* Word tokenize as well as parts of speech tag are imported from nltk\n",
    "* Default Dictionary is imported from collections\n",
    "* Dictionary is created where pos_tag (first letter) are the key values whose values are mapped with the value from wordnet dictionary. We have taken the only first letter as we will use it later in the loop.\n",
    "* Text is written and is tokenized.\n",
    "* Object lemma_function is created which will be used inside the loop\n",
    "* Loop is run and lemmatize will take two arguments one is token and other is a mapping of pos_tag with wordnet value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet with NLTK: Finding Synonyms for words in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Wordnet?\n",
    "Wordnet is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym. One can define it as a semantically oriented dictionary of English. It is imported with the following command:\n",
    "\n",
    ">from nltk.corpus import wordnet as guru\n",
    "\n",
    "Stats reveal that there are 155287 words and 117659 synonym sets included with English WordNet.\n",
    "\n",
    "Different methods available with WordNet can be found by typing dir(guru)\n",
    "\n",
    "`['_LazyCorpusLoader__args', '_LazyCorpusLoader__kwargs', '_LazyCorpusLoader__load', '_LazyCorpusLoader__name', '_LazyCorpusLoader__reader_cls', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_unload', 'subdir', 'unicode_repr']`\n",
    "\n",
    "Let us understand some of the features available with the wordnet:\n",
    "\n",
    "Synset: It is also called as synonym set or collection of synonym words. Let us check a example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"dog\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexical Relations**: These are semantic relations which are reciprocated. If there is a relationship between {x1,x2,...xn} and {y1,y2,...yn} then there is also relation between {y1,y2,...yn} and {x1,x2,...xn}. For example Synonym is the opposite of antonym or hypernyms and hyponym are type of lexical concept.\n",
    "\n",
    "Let us write a program using python to find synonym and antonym of word \"active\" using Wordnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'fighting', 'active', 'participating', 'active_voice', 'alive', 'combat-ready', 'dynamic', 'active_agent'}\n{'stative', 'passive_voice', 'passive', 'extinct', 'inactive', 'dormant', 'quiet'}\n"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets(\"active\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(set(synonyms))\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the code**\n",
    "\n",
    "* Wordnet is a corpus, so it is imported from the ntlk.corpus\n",
    "* List of both synonym and antonym is taken as empty which will be used for appending\n",
    "* Synonyms of the word active are searched in the module synsets and are appended in the list synonyms. The same process is repeated for the second one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "WordNet is a lexical database that has been used by a major search engine. From the WordNet, information about a given word or phrase can be calculated such as\n",
    "\n",
    "* synonym (words having the same meaning)\n",
    "* hypernyms (The generic term used to designate a class of specifics (i.e., meal is a breakfast), hyponyms (rice is a meal)\n",
    "* holonyms (proteins, carbohydrates are part of meal)\n",
    "* meronyms (meal is part of daily food intake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet also provides information on co-ordinate terms, derivates, senses and more. It is used to find the similarities between any two words. It also holds information on the results of the related word. In short or nutshell one can treat it as Dictionary or Thesaurus. Going deeper in wordnet, it is divided into four total subnets such as\n",
    "\n",
    "* Noun\n",
    "* Verb\n",
    "* Adjective\n",
    "* Adverb\n",
    "\n",
    "It can be used in the area of artificial intelligence for text analysis. With the help of Wordnet, you can create your corpus for spelling checking, language translation, Spam detection and many more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting POS Tags, Frequency Distribution & Collocations in NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COUNTING POS TAGS\n",
    "We have discussed various pos_tag in the previous section. In this particular tutorial, you will study how to count these tags. Counting tags are crucial for text classification as well as preparing the features for the Natural language-based operations. I will be discussing with you the approach which guru99 followed while preparing code along with a discussion of output. Hope this will help you.\n",
    "\n",
    "How to count Tags:\n",
    "\n",
    "Here first we will write working code and then we will write different steps to explain the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Counter({'NN': 5, ',': 2, 'VBZ': 1, 'CD': 1, 'IN': 1, 'DT': 1, 'JJS': 1, 'NNS': 1, 'TO': 1, 'VB': 1, 'JJ': 1, 'CC': 1, 'RB': 1, 'JJR': 1, '.': 1})\n"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "text = \" Guru99 is one of the best sites to learn WEB, SAP, Ethical Hacking and much more online.\"\n",
    "lower_case = text.lower()\n",
    "tokens = nltk.word_tokenize(lower_case)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "counts = Counter( tag for word,  tag in tags)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "* To count the tags, you can use the package Counter from the collection's module. A counter is a dictionary subclass which works on the principle of key-value operation. It is an unordered collection where elements are stored as a dictionary key while the count is their value.\n",
    "* Import nltk which contains modules to tokenize the text.\n",
    "* Write the text whose pos_tag you want to count.\n",
    "* Some words are in upper case and some in lower case, so it is appropriate to transform all the words in the lower case before applying tokenization.\n",
    "* Pass the words through word_tokenize from nltk.\n",
    "* Calculate the pos_tag of each token\n",
    "\n",
    "`Output = [('guru99', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('site', 'NN'), ('to', 'TO'), ('learn', 'VB'), ('web', 'NN'), (',', ','), ('sap', 'NN'), (',', ','), ('ethical', 'JJ'), ('hacking', 'NN'), ('and', 'CC'), ('much', 'RB'), ('more', 'JJR'), ('online', 'JJ')]\t`\t\t\t\t\n",
    "* Now comes the role of dictionary counter. We have imported in the code line 1. Words are the key and tags are the value and counter will count each tag total count present in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution\n",
    "\n",
    "Frequency Distribution is referred to as the number of times an outcome of an experiment occurs. It is used to find the frequency of each word occurring in a document. It uses FreqDistclass and defined by the nltk.probabilty module.\n",
    "\n",
    "A frequency distribution is usually created by counting the samples of repeatedly running the experiment. The no of counts is incremented by one, each time. E.g.\n",
    "\n",
    "`freq_dist = FreqDist()`\n",
    "\n",
    "for the token in the document:\n",
    "\n",
    "`freq_dist.inc(token.type())`\n",
    "\n",
    "For any word, we can check how many times it occurred in a particular document. E.g.\n",
    "\n",
    "* Count Method: freq_dist.count('and')This expression returns the value of the number of times 'and' occurred. It is called the count method.\n",
    "* Frequency Method: freq_dist.freq('and')This the expression returns frequency of a given sample.\n",
    "We will write a small program and will explain its working in detail. \n",
    "\n",
    "We will write some text and will calculate the frequency distribution of each word in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"312.913437pt\" version=\"1.1\" viewBox=\"0 0 392.14375 312.913437\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 312.913437 \r\nL 392.14375 312.913437 \r\nL 392.14375 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 50.14375 224.64 \r\nL 384.94375 224.64 \r\nL 384.94375 7.2 \r\nL 50.14375 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 65.361932 224.64 \r\nL 65.361932 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_2\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m9504701524\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- the -->\r\n      <defs>\r\n       <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n      </defs>\r\n      <g transform=\"translate(68.121307 248.050937)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"102.587891\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_3\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 77.536477 224.64 \r\nL 77.536477 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"77.536477\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- for -->\r\n      <defs>\r\n       <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      </defs>\r\n      <g transform=\"translate(80.295852 245.39)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"35.205078\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"96.386719\" xlink:href=\"#DejaVuSans-114\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_5\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 89.711023 224.64 \r\nL 89.711023 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"89.711023\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- . -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(92.470398 234.818125)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-46\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_7\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 101.885568 224.64 \r\nL 101.885568 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"101.885568\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- site -->\r\n      <defs>\r\n       <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n       <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n      </defs>\r\n      <g transform=\"translate(104.644943 249.700937)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"79.882812\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"119.091797\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_9\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 114.060114 224.64 \r\nL 114.060114 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.060114\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- Tutorial -->\r\n      <defs>\r\n       <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n       <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n       <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n       <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      </defs>\r\n      <g transform=\"translate(116.819489 269.89625)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-84\"/>\r\n       <use x=\"60.849609\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"124.228516\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"163.4375\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"224.619141\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"265.732422\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"293.515625\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"354.794922\" xlink:href=\"#DejaVuSans-108\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_11\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 126.234659 224.64 \r\nL 126.234659 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.234659\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- Beginners -->\r\n      <defs>\r\n       <path d=\"M 19.671875 34.8125 \r\nL 19.671875 8.109375 \r\nL 35.5 8.109375 \r\nQ 43.453125 8.109375 47.28125 11.40625 \r\nQ 51.125 14.703125 51.125 21.484375 \r\nQ 51.125 28.328125 47.28125 31.5625 \r\nQ 43.453125 34.8125 35.5 34.8125 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 42.828125 \r\nL 34.28125 42.828125 \r\nQ 41.5 42.828125 45.03125 45.53125 \r\nQ 48.578125 48.25 48.578125 53.8125 \r\nQ 48.578125 59.328125 45.03125 62.0625 \r\nQ 41.5 64.796875 34.28125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 35.015625 72.90625 \r\nQ 46.296875 72.90625 52.390625 68.21875 \r\nQ 58.5 63.53125 58.5 54.890625 \r\nQ 58.5 48.1875 55.375 44.234375 \r\nQ 52.25 40.28125 46.1875 39.3125 \r\nQ 53.46875 37.75 57.5 32.78125 \r\nQ 61.53125 27.828125 61.53125 20.40625 \r\nQ 61.53125 10.640625 54.890625 5.3125 \r\nQ 48.25 0 35.984375 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-66\"/>\r\n       <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      </defs>\r\n      <g transform=\"translate(128.994034 281.929062)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-66\"/>\r\n       <use x=\"68.603516\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"130.126953\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"193.603516\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"221.386719\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"284.765625\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"348.144531\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"409.667969\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"450.78125\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_13\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 138.409205 224.64 \r\nL 138.409205 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"138.409205\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- and -->\r\n      <defs>\r\n       <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n      </defs>\r\n      <g transform=\"translate(141.16858 250.454062)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"61.279297\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-100\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_15\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 150.58375 224.64 \r\nL 150.58375 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"150.58375\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- much -->\r\n      <defs>\r\n       <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      </defs>\r\n      <g transform=\"translate(153.343125 259.554062)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"97.412109\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"160.791016\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"215.771484\" xlink:href=\"#DejaVuSans-104\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_17\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 162.758295 224.64 \r\nL 162.758295 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"162.758295\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- more -->\r\n      <g transform=\"translate(165.51767 257.760312)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"158.59375\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"199.675781\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_19\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 174.932841 224.64 \r\nL 174.932841 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.932841\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- Guru99 -->\r\n      <defs>\r\n       <path d=\"M 59.515625 10.40625 \r\nL 59.515625 29.984375 \r\nL 43.40625 29.984375 \r\nL 43.40625 38.09375 \r\nL 69.28125 38.09375 \r\nL 69.28125 6.78125 \r\nQ 63.578125 2.734375 56.6875 0.65625 \r\nQ 49.8125 -1.421875 42 -1.421875 \r\nQ 24.90625 -1.421875 15.25 8.5625 \r\nQ 5.609375 18.5625 5.609375 36.375 \r\nQ 5.609375 54.25 15.25 64.234375 \r\nQ 24.90625 74.21875 42 74.21875 \r\nQ 49.125 74.21875 55.546875 72.453125 \r\nQ 61.96875 70.703125 67.390625 67.28125 \r\nL 67.390625 56.78125 \r\nQ 61.921875 61.421875 55.765625 63.765625 \r\nQ 49.609375 66.109375 42.828125 66.109375 \r\nQ 29.4375 66.109375 22.71875 58.640625 \r\nQ 16.015625 51.171875 16.015625 36.375 \r\nQ 16.015625 21.625 22.71875 14.15625 \r\nQ 29.4375 6.6875 42.828125 6.6875 \r\nQ 48.046875 6.6875 52.140625 7.59375 \r\nQ 56.25 8.5 59.515625 10.40625 \r\nz\r\n\" id=\"DejaVuSans-71\"/>\r\n       <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n      </defs>\r\n      <g transform=\"translate(177.692216 268.899375)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-71\"/>\r\n       <use x=\"77.490234\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"140.869141\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"181.982422\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"245.361328\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"308.984375\" xlink:href=\"#DejaVuSans-57\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_11\">\r\n     <g id=\"line2d_21\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 187.107386 224.64 \r\nL 187.107386 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.107386\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- is -->\r\n      <g transform=\"translate(189.866761 239.6275)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"27.783203\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_12\">\r\n     <g id=\"line2d_23\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 199.281932 224.64 \r\nL 199.281932 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"199.281932\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- where -->\r\n      <defs>\r\n       <path d=\"M 4.203125 54.6875 \r\nL 13.1875 54.6875 \r\nL 24.421875 12.015625 \r\nL 35.59375 54.6875 \r\nL 46.1875 54.6875 \r\nL 57.421875 12.015625 \r\nL 68.609375 54.6875 \r\nL 77.59375 54.6875 \r\nL 63.28125 0 \r\nL 52.6875 0 \r\nL 40.921875 44.828125 \r\nL 29.109375 0 \r\nL 18.5 0 \r\nz\r\n\" id=\"DejaVuSans-119\"/>\r\n      </defs>\r\n      <g transform=\"translate(202.041307 262.569687)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"81.787109\" xlink:href=\"#DejaVuSans-104\"/>\r\n       <use x=\"145.166016\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"206.689453\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"247.771484\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_13\">\r\n     <g id=\"line2d_25\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 211.456477 224.64 \r\nL 211.456477 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_26\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.456477\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- you -->\r\n      <defs>\r\n       <path d=\"M 32.171875 -5.078125 \r\nQ 28.375 -14.84375 24.75 -17.8125 \r\nQ 21.140625 -20.796875 15.09375 -20.796875 \r\nL 7.90625 -20.796875 \r\nL 7.90625 -13.28125 \r\nL 13.1875 -13.28125 \r\nQ 16.890625 -13.28125 18.9375 -11.515625 \r\nQ 21 -9.765625 23.484375 -3.21875 \r\nL 25.09375 0.875 \r\nL 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 11.921875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nz\r\n\" id=\"DejaVuSans-121\"/>\r\n      </defs>\r\n      <g transform=\"translate(214.215852 250.015)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-121\"/>\r\n       <use x=\"59.179688\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"120.361328\" xlink:href=\"#DejaVuSans-117\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_14\">\r\n     <g id=\"line2d_27\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 223.631023 224.64 \r\nL 223.631023 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_28\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"223.631023\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- can -->\r\n      <g transform=\"translate(226.390398 249.604062)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"54.980469\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"116.259766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_15\">\r\n     <g id=\"line2d_29\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 235.805568 224.64 \r\nL 235.805568 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_30\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.805568\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- find -->\r\n      <g transform=\"translate(238.564943 250.624375)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"35.205078\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"62.988281\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"126.367188\" xlink:href=\"#DejaVuSans-100\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_16\">\r\n     <g id=\"line2d_31\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 247.980114 224.64 \r\nL 247.980114 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_32\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"247.980114\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- best -->\r\n      <defs>\r\n       <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n      </defs>\r\n      <g transform=\"translate(250.739489 253.27125)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"125\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"177.099609\" xlink:href=\"#DejaVuSans-116\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_17\">\r\n     <g id=\"line2d_33\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 260.154659 224.64 \r\nL 260.154659 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_34\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"260.154659\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- tutorials -->\r\n      <g transform=\"translate(262.914034 272.941562)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"39.208984\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"102.587891\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"141.796875\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"202.978516\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"244.091797\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"271.875\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"333.154297\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"360.9375\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_18\">\r\n     <g id=\"line2d_35\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 272.329205 224.64 \r\nL 272.329205 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_36\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"272.329205\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- Software -->\r\n      <defs>\r\n       <path d=\"M 53.515625 70.515625 \r\nL 53.515625 60.890625 \r\nQ 47.90625 63.578125 42.921875 64.890625 \r\nQ 37.9375 66.21875 33.296875 66.21875 \r\nQ 25.25 66.21875 20.875 63.09375 \r\nQ 16.5 59.96875 16.5 54.203125 \r\nQ 16.5 49.359375 19.40625 46.890625 \r\nQ 22.3125 44.4375 30.421875 42.921875 \r\nL 36.375 41.703125 \r\nQ 47.40625 39.59375 52.65625 34.296875 \r\nQ 57.90625 29 57.90625 20.125 \r\nQ 57.90625 9.515625 50.796875 4.046875 \r\nQ 43.703125 -1.421875 29.984375 -1.421875 \r\nQ 24.8125 -1.421875 18.96875 -0.25 \r\nQ 13.140625 0.921875 6.890625 3.21875 \r\nL 6.890625 13.375 \r\nQ 12.890625 10.015625 18.65625 8.296875 \r\nQ 24.421875 6.59375 29.984375 6.59375 \r\nQ 38.421875 6.59375 43.015625 9.90625 \r\nQ 47.609375 13.234375 47.609375 19.390625 \r\nQ 47.609375 24.75 44.3125 27.78125 \r\nQ 41.015625 30.8125 33.5 32.328125 \r\nL 27.484375 33.5 \r\nQ 16.453125 35.6875 11.515625 40.375 \r\nQ 6.59375 45.0625 6.59375 53.421875 \r\nQ 6.59375 63.09375 13.40625 68.65625 \r\nQ 20.21875 74.21875 32.171875 74.21875 \r\nQ 37.3125 74.21875 42.625 73.28125 \r\nQ 47.953125 72.359375 53.515625 70.515625 \r\nz\r\n\" id=\"DejaVuSans-83\"/>\r\n      </defs>\r\n      <g transform=\"translate(275.08858 276.113437)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-83\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"124.658203\" xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"159.847656\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"199.056641\" xlink:href=\"#DejaVuSans-119\"/>\r\n       <use x=\"280.84375\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"342.123047\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"383.205078\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_19\">\r\n     <g id=\"line2d_37\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 284.50375 224.64 \r\nL 284.50375 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_38\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"284.50375\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_19\">\r\n      <!-- Testing -->\r\n      <g transform=\"translate(287.263125 268.468125)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-84\"/>\r\n       <use x=\"60.818359\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"122.341797\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"174.441406\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"213.650391\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"241.433594\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"304.8125\" xlink:href=\"#DejaVuSans-103\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_20\">\r\n     <g id=\"line2d_39\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 296.678295 224.64 \r\nL 296.678295 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_40\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.678295\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_20\">\r\n      <!-- , -->\r\n      <defs>\r\n       <path d=\"M 11.71875 12.40625 \r\nL 22.015625 12.40625 \r\nL 22.015625 4 \r\nL 14.015625 -11.625 \r\nL 7.71875 -11.625 \r\nL 11.71875 4 \r\nz\r\n\" id=\"DejaVuSans-44\"/>\r\n      </defs>\r\n      <g transform=\"translate(299.43767 234.818125)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-44\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_21\">\r\n     <g id=\"line2d_41\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 308.852841 224.64 \r\nL 308.852841 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_42\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"308.852841\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_21\">\r\n      <!-- SAP -->\r\n      <defs>\r\n       <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n       <path d=\"M 19.671875 64.796875 \r\nL 19.671875 37.40625 \r\nL 32.078125 37.40625 \r\nQ 38.96875 37.40625 42.71875 40.96875 \r\nQ 46.484375 44.53125 46.484375 51.125 \r\nQ 46.484375 57.671875 42.71875 61.234375 \r\nQ 38.96875 64.796875 32.078125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 32.078125 72.90625 \r\nQ 44.34375 72.90625 50.609375 67.359375 \r\nQ 56.890625 61.8125 56.890625 51.125 \r\nQ 56.890625 40.328125 50.609375 34.8125 \r\nQ 44.34375 29.296875 32.078125 29.296875 \r\nL 19.671875 29.296875 \r\nL 19.671875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-80\"/>\r\n      </defs>\r\n      <g transform=\"translate(311.612216 250.860312)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-83\"/>\r\n       <use x=\"63.492188\" xlink:href=\"#DejaVuSans-65\"/>\r\n       <use x=\"131.900391\" xlink:href=\"#DejaVuSans-80\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_22\">\r\n     <g id=\"line2d_43\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 321.027386 224.64 \r\nL 321.027386 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_44\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"321.027386\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_22\">\r\n      <!-- Course -->\r\n      <defs>\r\n       <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n      </defs>\r\n      <g transform=\"translate(323.786761 266.5525)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-67\"/>\r\n       <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"194.384766\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"235.498047\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"287.597656\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_23\">\r\n     <g id=\"line2d_45\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 333.201932 224.64 \r\nL 333.201932 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_46\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"333.201932\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_23\">\r\n      <!-- Java -->\r\n      <defs>\r\n       <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 5.078125 \r\nQ 19.671875 -8.109375 14.671875 -14.0625 \r\nQ 9.671875 -20.015625 -1.421875 -20.015625 \r\nL -5.171875 -20.015625 \r\nL -5.171875 -11.71875 \r\nL -2.09375 -11.71875 \r\nQ 4.4375 -11.71875 7.125 -8.046875 \r\nQ 9.8125 -4.390625 9.8125 5.078125 \r\nz\r\n\" id=\"DejaVuSans-74\"/>\r\n       <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      </defs>\r\n      <g transform=\"translate(335.961307 252.765)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-74\"/>\r\n       <use x=\"29.492188\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"90.771484\" xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"149.951172\" xlink:href=\"#DejaVuSans-97\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_24\">\r\n     <g id=\"line2d_47\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 345.376477 224.64 \r\nL 345.376477 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_48\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"345.376477\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_24\">\r\n      <!-- Please -->\r\n      <g transform=\"translate(348.135852 264.091563)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-80\"/>\r\n       <use x=\"60.302734\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"88.085938\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"149.609375\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"210.888672\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"262.988281\" xlink:href=\"#DejaVuSans-101\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_25\">\r\n     <g id=\"line2d_49\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 357.551023 224.64 \r\nL 357.551023 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_50\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"357.551023\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_25\">\r\n      <!-- visit -->\r\n      <g transform=\"translate(360.310398 252.244687)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-118\"/>\r\n       <use x=\"59.179688\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"86.962891\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"139.0625\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"166.845703\" xlink:href=\"#DejaVuSans-116\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_26\">\r\n     <g id=\"line2d_51\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 369.725568 224.64 \r\nL 369.725568 7.2 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_52\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.725568\" xlink:href=\"#m9504701524\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_26\">\r\n      <!-- guru99.com -->\r\n      <g transform=\"translate(372.484943 292.035312)rotate(-90)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"126.855469\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"167.96875\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"231.347656\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"294.970703\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"358.59375\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"390.380859\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"445.361328\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"506.542969\" xlink:href=\"#DejaVuSans-109\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_27\">\r\n     <!-- Samples -->\r\n     <defs>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n     </defs>\r\n     <g transform=\"translate(196.190625 303.63375)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-83\"/>\r\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-109\"/>\r\n      <use x=\"222.167969\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"285.644531\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"313.427734\" xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"374.951172\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_53\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 214.756364 \r\nL 384.94375 214.756364 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_54\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m14ebbc3c32\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"214.756364\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_28\">\r\n      <!-- 1.00 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 218.555582)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_55\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 190.047273 \r\nL 384.94375 190.047273 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_56\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"190.047273\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_29\">\r\n      <!-- 1.25 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 193.846491)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_57\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 165.338182 \r\nL 384.94375 165.338182 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_58\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"165.338182\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_30\">\r\n      <!-- 1.50 -->\r\n      <g transform=\"translate(20.878125 169.137401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_59\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 140.629091 \r\nL 384.94375 140.629091 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_60\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"140.629091\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_31\">\r\n      <!-- 1.75 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 144.42831)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_61\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 115.92 \r\nL 384.94375 115.92 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_62\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"115.92\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_32\">\r\n      <!-- 2.00 -->\r\n      <g transform=\"translate(20.878125 119.719219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_63\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 91.210909 \r\nL 384.94375 91.210909 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_64\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"91.210909\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_33\">\r\n      <!-- 2.25 -->\r\n      <g transform=\"translate(20.878125 95.010128)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_65\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 66.501818 \r\nL 384.94375 66.501818 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_66\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"66.501818\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_34\">\r\n      <!-- 2.50 -->\r\n      <g transform=\"translate(20.878125 70.301037)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_67\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 41.792727 \r\nL 384.94375 41.792727 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_68\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"41.792727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_35\">\r\n      <!-- 2.75 -->\r\n      <g transform=\"translate(20.878125 45.591946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_69\">\r\n      <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 50.14375 17.083636 \r\nL 384.94375 17.083636 \r\n\" style=\"fill:none;stroke:#c0c0c0;stroke-linecap:square;stroke-width:0.8;\"/>\r\n     </g>\r\n     <g id=\"line2d_70\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m14ebbc3c32\" y=\"17.083636\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_36\">\r\n      <!-- 3.00 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 20.882855)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_37\">\r\n     <!-- Counts -->\r\n     <g transform=\"translate(14.798438 133.373125)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-67\"/>\r\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"131.005859\" xlink:href=\"#DejaVuSans-117\"/>\r\n      <use x=\"194.384766\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"257.763672\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"296.972656\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_71\">\r\n    <path clip-path=\"url(#pd89fbb6a4c)\" d=\"M 65.361932 17.083636 \r\nL 77.536477 17.083636 \r\nL 89.711023 17.083636 \r\nL 101.885568 115.92 \r\nL 114.060114 115.92 \r\nL 126.234659 115.92 \r\nL 138.409205 115.92 \r\nL 150.58375 115.92 \r\nL 162.758295 115.92 \r\nL 174.932841 214.756364 \r\nL 187.107386 214.756364 \r\nL 199.281932 214.756364 \r\nL 211.456477 214.756364 \r\nL 223.631023 214.756364 \r\nL 235.805568 214.756364 \r\nL 247.980114 214.756364 \r\nL 260.154659 214.756364 \r\nL 272.329205 214.756364 \r\nL 284.50375 214.756364 \r\nL 296.678295 214.756364 \r\nL 308.852841 214.756364 \r\nL 321.027386 214.756364 \r\nL 333.201932 214.756364 \r\nL 345.376477 214.756364 \r\nL 357.551023 214.756364 \r\nL 369.725568 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:2;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 50.14375 224.64 \r\nL 50.14375 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 384.94375 224.64 \r\nL 384.94375 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 50.14375 224.64 \r\nL 384.94375 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 50.14375 7.2 \r\nL 384.94375 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pd89fbb6a4c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.axes._subplots.AxesSubplot at 0x1fbfdf38708>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "import nltk\n",
    "a = \"Guru99 is the site where you can find the best tutorials for Software Testing     Tutorial, SAP Course for Beginners. Java Tutorial for Beginners and much more. Please     visit the site guru99.com and much more.\"\n",
    "words = nltk.tokenize.word_tokenize(a)\n",
    "fd = nltk.FreqDist(words)\n",
    "fd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Explanation of code:**\n",
    "\n",
    "* Import nltk module.\n",
    "* Write the text whose word distribution you need to find.\n",
    "* Tokenize each word in the text which is served as input to FreqDist module of the nltk.\n",
    "* Apply each word to nlk.FreqDist in the form of a list\n",
    "* Plot the words in the graph using plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the graph above. It corresponds to counting the occurrence of each word in the text. It helps in the study of text and further in implementing text-based sentimental analysis. In a nutshell, it can be concluded that nltk has a module for counting the occurrence of each word in the text which helps in preparing the stats of natural language features. It plays a significant role in finding the keywords in the text. You can also extract the text from the pdf using libraries like extract, PyPDF2 and feed the text to nlk.FreqDist.\n",
    "\n",
    "The key term is \"tokenize.\" After tokenizing, it checks for each word in a given paragraph or text document to determine that number of times it occurred. You do not need the NLTK toolkit for this. You can also do it with your own python programming skills. NLTK toolkit only provides a ready-to-use code for the various operations.\n",
    "\n",
    "Counting each word may not be much useful. Instead one should focus on collocation and bigrams which deals with a lot of words in a pair. These pairs identify useful keywords to better natural language features which can be fed to the machine. Please look below for their details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations: Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Collocations?\n",
    "Collocations are the pairs of words occurring together many times in a document. It is calculated by the number of those pair occurring together to the overall word count of the document.\n",
    "\n",
    "Consider electromagnetic spectrum with words like ultraviolet rays, infrared rays.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words ultraviolet and rays are not used individually and hence can be treated as Collocation. Another example is the CT Scan. We don't say CT and Scan separately, and hence they are also treated as collocation.\n",
    "\n",
    "We can say that finding collocations requires calculating the frequencies of words and their appearance in the context of other words. These specific collections of words require filtering to retain useful content terms. Each gram of words may then be scored according to some association measure, to determine the relative likelihood of each Ingram being a collocation.\n",
    "\n",
    "Collocation can be categorized into two types-\n",
    "\n",
    "* Bigrams combination of two words\n",
    "* Trigramscombinationof three words\n",
    "\n",
    "Bigrams and Trigrams provide more meaningful and useful features for the feature extraction stage. These are especially useful in text-based sentimental analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigrams Example Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('Guru99', 'is'), ('is', 'a'), ('a', 'totally'), ('totally', 'new'), ('new', 'kind'), ('kind', 'of'), ('of', 'learning'), ('learning', 'experience'), ('experience', '.')]\n"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Guru99 is a totally new kind of learning experience.\"\n",
    "Tokens = nltk.word_tokenize(text)\n",
    "output = list(nltk.bigrams(Tokens))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trigrams Example Code**\n",
    "\n",
    "Sometimes it becomes important to see a pair of three words in the sentence for statistical analysis and frequency count. This again plays a crucial role in forming NLP (natural language processing features) as well as text-based sentimental prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('Guru99', 'is', 'a'), ('is', 'a', 'totally'), ('a', 'totally', 'new'), ('totally', 'new', 'kind'), ('new', 'kind', 'of'), ('kind', 'of', 'learning'), ('of', 'learning', 'experience'), ('learning', 'experience', '.')]\n"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Guru99 is a totally new kind of learning experience.\"\n",
    "Tokens = nltk.word_tokenize(text)\n",
    "output = list(nltk.trigrams(Tokens))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Tutorial: word2vec using Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word Embedding?\n",
    "Word Embedding is a type of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix. It is language modeling and feature learning technique. Word embedding is a way to perform mapping using a neural network. There are various word embedding models available such as word2vec (Google), Glove (Stanford) and fastest (Facebook).\n",
    "\n",
    "Word Embedding is also called as distributed semantic model or distributed represented or semantic vector space or vector space model. As you read these names, you come across the word semantic which means categorizing similar words together. For example fruits like apple, mango, banana should be placed close whereas books will be far away from these words. In a broader sense, word embedding will create the vector of fruits which will be placed far away from vector representation of books."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where is Word Embedding used?\n",
    "Word embedding helps in feature generation, document clustering, text classification, and natural language processing tasks. Let us list them and have some discussion on each of these applications.\n",
    "\n",
    "* **Compute similar words**: Word embedding is used to suggest similar words to the word being subjected to the prediction model. Along with that it also suggests dissimilar words, as well as most common words.\n",
    "Create a group of related words: It is used for semantic grouping which will group things of similar characteristic together and dissimilar far away.\n",
    "* **Feature for text classification**: Text is mapped into arrays of vectors which is fed to the model for training as well as prediction. Text-based classifier models cannot be trained on the string, so this will convert the text into machine trainable form. Further its features of building semantic help in text-based classification.\n",
    "* **Document clustering** is another application where word embedding is widely used\n",
    "* **Natural language processing**: There are many applications where word embedding is useful and wins over feature extraction phases such as parts of speech tagging, sentimental analysis, and syntactic analysis.\n",
    "Now we have got some knowledge of word embedding. Some light is also thrown on different models to implement word embedding. This whole tutorial is focused on one of the models (word2vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is word2vec?\n",
    "Word2vec is the technique/model to produce word embedding for better word representation. It captures a large number of precise syntactic and semantic word relationship. It is a shallow two-layered neural network. Before going further, please see the difference between shallow and deep neural network:\n",
    "\n",
    "The shallow neural network consists of the only a hidden layer between input and output whereas deep neural network contains multiple hidden layers between input and output. Input is subjected to nodes whereas the hidden layer, as well as the output layer, contains neurons.\n",
    "\n",
    "![](111318_0826_WordEmbeddi1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What word2vec does?\n",
    "\n",
    "Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away. This is also termed as a semantic relationship. Neural networks do not understand text instead they understand only numbers. Word Embedding provides a way to convert text to a numeric vector.\n",
    "\n",
    "Word2vec reconstructs the linguistic context of words. Before going further let us understand, what is linguistic context? In general life scenario when we speak or write to communicate, other people try to figure out what is objective of the sentence. For example, \"What is the temperature of India\", here the context is the user wants to know \"temperature of India\" which is context. In short, the main objective of a sentence is context. Word or sentence surrounding spoken or written language (disclosure) helps in determining the meaning of context. Word2vec learns vector representation of words through the contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Word2vec?\n",
    "\n",
    "#### Before Word Embedding\n",
    "It is important to know which approach is used before word embedding and what are its demerits and then we will move to the topic of how demerits are overcome by Word embedding using word2vec approach. Finally, we will move how word2vec works because it is important to understand it's working.\n",
    "\n",
    "#### Approach for Latent Semantic Analysis\n",
    "This is the approach which was used before word embedding. It used the concept of Bag of words where words are represented in the form of encoded vectors. It is a sparse vector representation where the dimension is equal to the size of vocabulary. If the word occurs in the dictionary, it is counted, else not. To understand more, please see the below program.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 2 1 1 1 1 1 1 1 1]]\n['best', 'guru99', 'is', 'love', 'online', 'sitefor', 'the', 'to', 'tutorials', 'visit']\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer()\n",
    "data_corpus=[\"guru99 is the best sitefor online tutorials. I love to visit guru99.\"]\n",
    "vocabulary=vectorizer.fit(data_corpus)\n",
    "X= vectorizer.transform(data_corpus)\n",
    "print(X.toarray())\n",
    "print(vocabulary.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "* CountVectorizer is the module which is used to store the vocabulary based on fitting the words in it. This is imported from the sklearn\n",
    "* Make the object using the class CountVectorizer.\n",
    "* Write the data in the list which is to be fitted in the CountVectorizer.\n",
    "* Data is fit in the object created from the class CountVectorizer.\n",
    "* Apply a bag of word approach to count words in the data using vocabulary. If word or token is not available in the vocabulary, then such index position is set to zero.\n",
    "* Variable in line 5 which is x is converted to an array (method available for x). This will provide the count of each token in the sentence or list provided in Line 3.\n",
    "* This will show the features which are part of the vocabulary when it is fitted using the data in Line 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Latent Semantic approach, the row represents unique words whereas the column represents the number of time that word appears in the document. It is a representation of words in the form of the document matrix. Term-Frequency inverse document frequency (TFIDF) is used to count the frequency of words in the document which is the frequency of the term in the document/ frequency of the term in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcoming of Bag of Words method\n",
    "* It ignores the order of the word, for example, this is bad = bad is this.\n",
    "* It ignores the context of words. Suppose If I write the sentence \"He loved books. Education is best found in books\". It would create two vectors one for \"He loved books\" and other for \"Education is best found in books.\" It would treat both of them orthogonal which makes them independent, but in reality, they are related to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Word2vec works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec learns word by predicting its surrounding context. For example, let us take the word \"He loves Football.\"\n",
    "\n",
    "We want to calculate the word2vec for the word: loves.\n",
    "\n",
    "Suppose\n",
    "\n",
    ">loves =  Vin. P(Vout / Vin) is calculated\t\n",
    "\n",
    ">where,\t\n",
    "\n",
    ">Vin is the input word. \t\n",
    "\n",
    ">P is the probability of likelihood.\t\n",
    "\n",
    ">Vout is the output word. \t\n",
    "\n",
    "Word **loves** moves over each word in the corpus. Syntactic as well as the Semantic relationship between words is encoded. This helps in finding similar and analogies words.\n",
    "\n",
    "All random features of the word **loves** is calculated. These features are changed or update concerning neighbor or context words with the help of a back propagation method.\n",
    "\n",
    "Another way of learning is that if the context of two words are similar or two words have similar features, then such words are related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec Architecture\n",
    "There are two architectures used by word2vec\n",
    "\n",
    "* Continuous Bag of words (CBOW)\n",
    "* skip gram\n",
    "\n",
    "Before going further, let us discuss why these architectures or models are important from word representation point of view. Learning word representation is essentially unsupervised, but targets/labels are needed to train the model. Skip-gram and CBOW convert unsupervised representation to supervised form for model training.\n",
    "\n",
    "In CBOW, the current word is predicted using the window of surrounding context windows. For example, if wi-1,wi-2,wi+1,wi+2are given words or context, this model will provide wi\n",
    "\n",
    "Skip-Gram performs opposite of CBOW which implies that it predicts the given sequence or context from the word. You can reverse the example to understand it. If wi is given, this will predict the context or wi-1,wi-2,wi+1,wi+2.\n",
    "\n",
    "Word2vec provides an option to choose between CBOW (continuous Bag of words) and skim-gram. Such parameters are provided during training of the model. One can have the option of using negative sampling or hierarchical softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words.\n",
    "Let us draw a simple diagram to understand the continuous bag of word architecture.\n",
    "![](111318_0826_WordEmbeddi3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the equations mathematically. Suppose V is the vocabulary size and N is the hidden layer size. Input is defined as { xi-1, xi-2, xi+1, xi+2}. We obtain the weight matrix by multiplying V * N. Another matrix is obtained by multiplying input vector with the weight matrix. This can also be understood by the following equation.\n",
    "\n",
    "h=xitW\n",
    "\n",
    "where xit∧ W are the input vector and weight matrix respectively,\n",
    "\n",
    "To calculate the match between context and the next word, please refer to the below equation\n",
    "\n",
    "u=predictedrepresentation*h\n",
    "\n",
    "where predictedrepresentation is obtained model∧h in the above equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Gram Model\n",
    "Skip-Gram approach is used to predict a sentence given an input word. To understand it better let us draw the diagram.\n",
    "![](111318_0826_WordEmbeddi4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can treat it as the reverse of the Continuous bag of word model where the input is the word and model provides the context or the sequence. We can also conclude that the target is fed to the input and output layer is replicated multiple times to accommodate the chosen number of context words. Error vector from all the output layer is summed up to adjust weights via a backpropagation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The relation between Word2vec and NLTK\n",
    "NLTK is natural Language toolkit. It is used for preprocessing of the text. One can do different operations such as parts of speech tagging, lemmatizing, stemming, stop words removal, removing rare words or least used words. It helps in cleaning the text as well as helps in preparing the features from the effective words. In the other way, word2vec is used for semantic (closely related items together) and syntactic (sequence) matching. Using word2vec, one can find similar words, dissimilar words, dimensional reduction, and many others. Another important feature of word2vec is to convert the higher dimensional representation of the text into lower dimensional of vectors.\n",
    "\n",
    "#### Where to use NLTK and Word2vec?\n",
    "If one has to accomplish some general-purpose tasks as mentioned above like tokenization, POS tagging and parsing one must go for using NLTK whereas for predicting words according to some context, topic modeling, or document similarity one must use Word2vec.\n",
    "\n",
    "#### Relation of NLTK and Word2vec with the help of code\n",
    "NLTK and Word2vec can be used together to find similar words representation or syntactic matching. NLTK toolkit can be used to load many packages which come with NLTK and model can be created using word2vec. It can be then tested on the real time words. Let us see the combination of both in the following code. Before processing further, please have a look on the corpora which NLTK provides. You can download using the command nltk(nltk.download('all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('law', 0.9376879334449768), ('policy', 0.9264345765113831), ('agriculture', 0.9249087572097778), ('general', 0.9202353954315186), ('practice', 0.9194850921630859), ('media', 0.9156852960586548), ('board', 0.9134683609008789), ('discussion', 0.9121928215026855), ('Crean', 0.911756157875061), ('tight', 0.9097710847854614)]\n"
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= gensim.models.Word2Vec(abc.sents())\n",
    "X= list(model.wv.vocab)\n",
    "data=model.most_similar('science')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of Code**\n",
    "\n",
    "* nltk library is imported which from where you can download the abc corpus which we will use in the next step.\n",
    "* Gensim is imported. If Gensim is not installed, please install it using the command \" pip3 install gensim\". \n",
    "* import the corpus abc which has been downloaded using nltk.download('abc').\n",
    "* Pass the files to the model word2vec which is imported using Gensim as sentences.\n",
    "* Vocabulary is stored in the form of the variable.\n",
    "* Model is tested on sample word science as these files are related to science.\n",
    "* Here the similar word of \"science\" is predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activators and Word2Vec\n",
    "The activation function of the neuron defines the output of that neuron given a set of inputs. Biologically inspired by an activity in our brains where different neurons are activated using different stimuli. Let us understand the activation function through the following diagram.\n",
    "![](111318_0826_WordEmbeddi8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here x1,x2,..x4 is the node of the neural network.\n",
    "\n",
    "w1, w2, w3 is the weight of the node,\n",
    "\n",
    "∑ is the summation of all weight and node value which work as the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Activation function?\n",
    "If no activation function is used output would be linear but the functionality of linear function is limited. To achieve complex functionality such as object detection, image classification, typing text using voice and many other non-linear outputs is needed which is achieved using activation function.\n",
    "\n",
    "#### How the activation layer is computed in the word embedding (word2vec)\n",
    "Softmax Layer (normalized exponential function) is the output layer function which activates or fires each node. Another approach used is Hierarchical softmax where the complexity is calculated by O(log2V) wherein the softmax it is O(V) where V is the vocabulary size. The difference between these is the reduction of the complexity in hierarchical softmax layer. To understand its (Hierarchical softmax) functionality, please look at the below example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](111318_0826_WordEmbeddi9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to compute the probability of observing the word **love** given a certain context. The flow from the root to the leaf node will be the first move to node 2 and then to node 5. So if we have had the vocabulary size of 8, only three computations are needed. So it allows decomposing, calculation of the probability of one word (love)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What other options are available other than Hierarchical Softmax?\n",
    "If speaking in a general sense for word embedding options available are Differentiated Softmax, CNN-Softmax, Importance Sampling, Adaptive Importance sampling, Noise Contrastive Estimations, Negative Sampling, Self-Normalization, and infrequent Normalization.\n",
    "\n",
    "Speaking specifically about Word2vec we have negative sampling available.\n",
    "\n",
    "Negative Sampling is a way to sample the training data. It is somewhat like stochastic gradient descent, but with some difference. Negative sampling looks only for negative training examples. It is based on noise contrastive estimation and randomly samples words, not in the context. It is a fast training method and chooses the context randomly. If the predicted word appears in the randomly chosen context both the vectors are close to each other.\n",
    "\n",
    "#### What conclusion can be drawn?\n",
    "\n",
    "Activators are firing the neurons just like our neurons are fired using the external stimuli. Softmax layer is one of the output layer function which fires the neurons in case of word embedding. In word2vec we have options such as hierarchical softmax and negative sampling. Using activators, one can convert the linear function into the nonlinear function, and a complex machine learning algorithm can be implemented using such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Gensim?\n",
    "Gensim is a topic modeling toolkit which is implemented in python. Topic modeling is discovering hidden structure in the text body. Word2vec is imported from Gensim toolkit. Please note that Gensim not only provides an implementation of word2vec but also Doc2vec and FastText but this tutorial is all about word2vec so we will stick to the current topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of word2vec using Gensim\n",
    "Till now we have discussed what word2vec is, its different architectures, why there is a shift from a bag of words to word2vec, the relation between word2vec and NLTK with live code and activation functions. In this section, will implement word2vec using Gensim\n",
    "\n",
    "**Step 1) Data Collection**\n",
    "\n",
    "The first Step to implement any machine learning model or implementing natural language processing is data collection\n",
    "\n",
    "Please observe the data to build an intelligent chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"\"\"\n",
    "[{\"tag\": \"welcome\",\n",
    "\"patterns\": [\"Hi\", \"How are you\", \"Is any one to talk?\", \"Hello\", \"hi are you available\"],\n",
    "\"responses\": [\"Hello, thanks for contacting us\", \"Good to see you here\",\" Hi there, how may I assist you?\"]\n",
    "\n",
    "        },\n",
    "{\"tag\": \"goodbye\",\n",
    "\"patterns\": [\"Bye\", \"See you later\", \"Goodbye\", \"I will come back soon\"],\n",
    "\"responses\": [\"See you later, thanks for visiting\", \"have a great day ahead\", \"Wish you Come back again soon.\"]\n",
    "        },\n",
    "\n",
    "{\"tag\": \"thankful\",\n",
    "\"patterns\": [\"Thanks for helping me\", \"Thank your guidance\", \"That's helpful and kind from you\"],\n",
    "\"responses\": [\"Happy to help!\", \"Any time!\", \"My pleasure\", \"It is my duty to help you\"]\n",
    "        },\n",
    "        {\"tag\": \"hoursopening\",\n",
    "\"patterns\": [\"What hours are you open?\", \"Tell your opening time?\", \"When are you open?\", \"Just your timing please\"],\n",
    "\"responses\": [\"We're open every day 8am-7pm\", \"Our office hours are 8am-7pm every day\", \"We open office at 8 am and close at 7 pm\"]\n",
    "        },\n",
    "\n",
    "{\"tag\": \"payments\",\n",
    "\"patterns\": [\"Can I pay using credit card?\", \" Can I pay using Mastercard?\", \" Can I pay using cash only?\" ],\n",
    "\"responses\": [\"We accept VISA, Mastercard and credit card\", \"We accept credit card, debit cards and cash. Please don’t worry\"]\n",
    "        }\n",
    "   ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what we understand from the data\n",
    "\n",
    "* This data contains three things tag, pattern, and responses. The tag is the intent (what is the topic of discussion).\n",
    "* The data is in JSON format.\n",
    "* A pattern is a question users will ask to the bot\n",
    "* Responses is the answer that chatbot will provide to the corresponding question/pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2) Data preprocessing.**\n",
    "\n",
    "It is very important to process the raw data. If cleaned data is fed to the machine, then the model will respond more accurately and will learn the data more efficiently.\n",
    "\n",
    "This step involves removing stop words, stemming, unnecessary words, etc. Before going ahead, it is important to load data and convert it into a data frame. Please see the below code for such\n",
    "\n",
    "```\n",
    "import json\n",
    "json_file =’intents.json'\n",
    "with open('intents.json','r') as f:\n",
    "    data = json.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = json.loads(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of CODE**.\n",
    "\n",
    "* As data is in the form of json format hence json is imported\n",
    "* File is stored in the variable\n",
    "* File is open and loaded in data variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now data is imported and it is time to convert data into data frame. Please see the below code to see the next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "df['patterns'] = df['patterns'].apply(', '.join) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of CODE**\n",
    "\n",
    "1. Data is converted into data frame using pandas which was imported above.\n",
    "\n",
    "2. It will convert the list in column patterns to string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "import string\n",
    "stop = stopwords.words('english')\n",
    "df['patterns'] = df['patterns'].apply(lambda x:' '.join(x.lower() for x in x.split()))\n",
    "df['patterns']= df['patterns'].apply(lambda x: ' '.join(x for x in x.split() if x not in string.punctuation))\n",
    "df['patterns']= df['patterns'].str.replace('[^\\w\\s]','')\n",
    "df['patterns']= df['patterns'].apply(lambda x: ' '.join(x for x in x.split() if  not x.isdigit()))\n",
    "df['patterns'] = df['patterns'].apply(lambda x:' '.join(x for x in x.split() if not x in stop))\n",
    "df['patterns'] = df['patterns'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "\n",
    "1. English stop words are imported using stop word module from nltk toolkit\n",
    "\n",
    "2. All the words of the text is converted into lower case using for condition and lambda function. Lambda function is an anonymous function.\n",
    "\n",
    "3. All the rows of the text in the data frame is checked for string punctuations, and these are filtered.\n",
    "\n",
    "4. Characters such as numbers or dot are removed using a regular expression.\n",
    "\n",
    "5. Digits are removed from the text.\n",
    "\n",
    "6. Stop words are removed at this stage.\n",
    "\n",
    "7. Words are filtered now, and different form of the same word is removed using lemmatization. With these, we have finished the data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "            tag                                           patterns  \\\n0       welcome                     hi one talk hello hi available   \n1       goodbye               bye see later goodbye come back soon   \n2      thankful   thanks helping thank guidance thats helpful kind   \n3  hoursopening     hour open tell opening time open timing please   \n4      payments  pay using credit card pay using mastercard pay...   \n\n                                           responses  \n0  [Hello, thanks for contacting us, Good to see ...  \n1  [See you later, thanks for visiting, have a gr...  \n2  [Happy to help!, Any time!, My pleasure, It is...  \n3  [We're open every day 8am-7pm, Our office hour...  \n4  [We accept VISA, Mastercard and credit card, W...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tag</th>\n      <th>patterns</th>\n      <th>responses</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>welcome</td>\n      <td>hi one talk hello hi available</td>\n      <td>[Hello, thanks for contacting us, Good to see ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>goodbye</td>\n      <td>bye see later goodbye come back soon</td>\n      <td>[See you later, thanks for visiting, have a gr...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>thankful</td>\n      <td>thanks helping thank guidance thats helpful kind</td>\n      <td>[Happy to help!, Any time!, My pleasure, It is...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>hoursopening</td>\n      <td>hour open tell opening time open timing please</td>\n      <td>[We're open every day 8am-7pm, Our office hour...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>payments</td>\n      <td>pay using credit card pay using mastercard pay...</td>\n      <td>[We accept VISA, Mastercard and credit card, W...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3) Neural Network building using word2vec**\n",
    "\n",
    "Now it is time to build a model using Gensim module word2vec. We have to import word2vec from Gensim. Let us do this, and then we will build and in the final stage we will check the model on real time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can successfully build the model using Word2Vec. Please refer to the next line of code to learn how to create the model using Word2Vec. Text is provided to the model in the form of a list so we will convert the text from data frame to list using the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigger_list=[]\n",
    "for i in df['patterns']:\n",
    "     li = list(i.split(\" \"))\n",
    "     Bigger_list.append(li)\t\n",
    "Model= Word2Vec(Bigger_list,min_count=1,size=300,workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanations of Code**\n",
    "\n",
    "1. Created the bigger_list where the inner list is appended. This is the format which is fed to the model Word2Vec.\n",
    "\n",
    "2. Loop is implemented, and each entry of the patterns column of the data frame is iterated.\n",
    "\n",
    "3. Each element of the column patterns is split and stored in the inner list li\n",
    "\n",
    "4. the Inner list is appended with the outer list.\n",
    "\n",
    "5. This list is provided to the Word2Vec model. Let us understand some of the parameters provided here\n",
    "\n",
    "Min_count: It will ignore all the words with a total frequency lower than this.\n",
    "\n",
    "Size: It tells the dimensionality of the word vectors.\n",
    "\n",
    "Workers: These are the threads to train the model\n",
    "\n",
    "There are also others options available, and some important ones are explained below\n",
    "\n",
    "Window: Maximum distance between the current and predicted word within a sentence.\n",
    "\n",
    "Sg: It is a training algorithm and 1 for skip-gram and 0 for a Continuous bag of words. We have discussed these in details in above.\n",
    "\n",
    "Hs: If this is 1 then we are using hierarchical softmax for training and if 0 then negative sampling is used.\n",
    "\n",
    "Alpha: Initial learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4) Model saving**\n",
    "\n",
    "Model can be saved in the form of bin and model form. Bin is the binary format. Please see the below lines to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")\n",
    "model.save(\"model.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation of the above code**\n",
    "\n",
    "1. Model is saved in the form of a .model file.\n",
    "\n",
    "2. model is saved in the form of .bin file\n",
    "\n",
    "We will use this model to do real time testing such as Similar words, dissimilar words, and most common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5) Loading model and performing real time testing**\n",
    "\n",
    "Model is loaded using below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to print the vocabulary from it is done using below command vocab = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "microlensing', 'founding', 'Observatory', 'dim', 'acts', 'defect', 'lens', 'alignment', 'galactic', 'Astrophysics', 'peat', 'Southeast', 'cousin', 'elusive', 'specimen', 'vertebrate', 'goby', 'maturity', 'P', 'skull', 'Psychology', 'Brody', 'psychologist', 'underwent', 'loud', 'pair', 'bonding', 'electrode', 'ion', 'craft', 'Propulsion', 'ESA', 'SMART', 'trips', 'electrodes', 'ions', 'plasma', 'beams', 'atoms', 'lightweight', 'tailor', 'Liddicoat', 'atomic', 'imaged', 'electromagnetic', 'rip', 'delicate', 'ancestor', 'dinosaur', 'dinosaurs', 'History', 'excavated', 'numerous', 'carnivorous', 'Triassic', 'jacket', 'curator', 'palaeontology', 'beak', 'beaks', 'skeleton', 'crocodilians', 'Effigia', 'ghost', 'invisible', 'Georgia', 'extinct', 'relatives', 'resembles', 'Jurassic', 'Berkeley', 'biomass', 'Writing', 'Imperial', 'plausible', 'stresses', 'Atkins', 'strictly', 'Chen', 'negligible', 'carbohydrate', 'lasted', 'insulin', 'coma', 'glucose', 'stomach', 'nutritionally', 'kidneys', 'copies', 'Academy', 'thirst', 'scan', 'sensations', 'circuits', 'sensation', 'mask', 'radioactive', 'isotopes', 'similarly', 'trail', 'migratory', 'da', 'Atlantic', 'juvenile', 'Cambridge', 'Cowie', 'floating', 'hurricane', 'Hawaiian', 'opossum', 'marsupial', 'surprisingly', 'Belov', 'sequenced', 'cluster', 'MHC', 'chromosomes', 'mammalian', 'Understanding', 'Gondwana', 'intermediate', 'Genome', 'earwax', 'mutation', 'mutant', 'northeast', 'transplanted', 'Her', 'transmission', 'southeast', 'organ', 'transplant', 'junk', 'antenna', 'SuitSat', 'figuring', 'Goddard', 'Radio', 'arrives', 'files', 'computers', 'variations', 'keyboard', 'burials', 'domestication', 'distinguish', '135', 'Archaeological', 'colonisation', 'Eurasia', 'Morey', 'archaeology', 'genomes', '7000', 'Swedish', 'archaeologists', 'neck', 'Dogs', 'Rhode', 'prehistoric', 'arranged', 'manipulating', 'millennia', 'mathematician', 'Applied', 'Mathematics', 'athletes', 'computing', 'computational', 'linear', 'curve', 'Roman', '1500', 'shifted', 'Brun', 'measurements', 'UB313', 'Xena', 'radiation', 'KBO', '1930', 'orbital', 'deg', 'Astronomical', 'mere', 'Either', 'Time', 'outermost', 'Planck', 'Astronomy', 'asteroids', 'archaeological', 'astronomer', 'warrior', 'mythology', 'screen', 'companion', 'beam', 'blog', 'cameras', 'Irvine', 'combining', 'San', 'interactive', 'diluted', 'Jefferies', 'TAP', 'peptides', 'PLoS', 'paradigm', 'molecule', 'Institutes', 'emitted', 'Impact', 'Neanderthals', 'Israeli', 'inhabited', 'hunted', 'hominids', 'Daniel', 'Connecticut', 'marrow', 'nutritional', 'nutritious', 'dietary', 'Still', 'disappearance', 'Genes', 'twins', 'spur', '&#', '214', 'tzi', 'Alps', 'mummy', 'possess', 'Archaeology', 'Austria', 'blade', 'ethnic', 'faeces', 'donors', 'Diego', 'viral', 'Plants', 'RNA', 'leukaemia', 'gastrointestinal', 'medications', 'questionnaire', 'devised', 'Disorders', 'explanations', 'reassuring', 'sensing', 'Computer', 'Fraunhofer', 'tone', 'gestures', 'behave', 'exhibition', 'variables', 'emotion', 'Stem', 'abnormalities', 'confined', 'vitro', 'fertilisation', 'IVF', 'muscular', 'Cram', 'Trounson', 'abnormality', 'implantation', 'implanted', 'specialises', 'bioethics', 'regenerate', 'sheet', 'altitudes', 'sheets', 'micrometres', 'pill', 'advocates', 'RU', 'abortion', 'miscarriages', 'mifepristone', 'Therapeutic', 'uterus', 'miscarriage', 'bleeding', 'procedure', 'allergic', 'complications', 'eu', 'identity', 'geographical', 'speciation', 'H', 'volcanic', 'interbreeding', 'clever', 'expedition', 'Foja', 'Helgen', 'clue', 'indigenous', 'bowerbird', 'ritual', '1979', 'impressive', 'Swift', 'Guanlong', 'tyrannosaur', 'crest', 'Tyrannosaurus', 'rex', 'Cretaceous', 'northwestern', 'explosions', 'fossilised', 'protruding', 'palaeontologists', 'rings', 'dragon', 'kissing', 'meningococcal', 'repel', 'sore', 'throat', 'epithelial', 'glandular', 'supernova', 'moments', 'Astrophysical', 'Burrows', 'Arizona', 'waves', 'exploding', 'supernovae', 'layers', 'simulate', 'vibrate', 'explode', 'frequencies', 'phenomena', 'antidepressants', 'lizards', 'Scanlon', 'preserved', 'skulls', 'composed', 'galaxies', 'trillion', 'arXiv', 'Cornell', 'Kepler', 'weigh', 'measuring', 'primates', 'Female', 'baboons', 'comfort', 'Sylvia', 'queen', 'Engh', 'grooming', 'tended', 'rarely', 'postdoctoral', 'Pennsylvania', 'Eventually', 'socially', 'protects', 'infants', 'Advanced', 'chimpanzees', 'astrophysics', 'cosmic', 'melting', 'velocity', 'Telescope', 'invasion', 'pulsar', 'emits', 'bursts', 'RRATs', 'rotating', 'pulsars', 'pulses', 'Manchester', 'neutron', 'pulse', 'Evolution', 'censorship', 'commentators', 'Diesendorf', 'AIDS', 'vocal', 'voices', 'babies', 'abstract', 'numerical', 'monkeys', 'hormonal', 'insomnia', 'menstruation', 'disorder', 'disturbances', 'menstrual', 'physiological', 'deprivation', 'voodoo', 'Benin', 'tear', 'gods', 'Officials', 'rituals', 'centuries', 'notably', 'Caribbean', 'dancing', 'copyright', 'EFA', 'Kazaa', 'inappropriately', 'ARIA', 'Google', 'monitored', 'terraces', 'Alangui', 'Auckland', 'Kankanaey', 'dynamics', 'Maori', 'interacting', 'Saturn', 'Cassini', 'Ingersoll', 'interference', 'lighting', 'instrument', 'gigantic', 'unconscious', 'deliberation', 'puzzles', 'Cuthbertson', 'stimulation', 'eventual', 'Organization', 'elderly', 'capsules', 'bubbles', 'Tuch', 'islets', 'blowing', 'antibodies', 'cytokines', 'therapies', 'homing', 'resemble', 'Advancement', 'Carnegie', 'Institution', 'constellation', 'alpha', 'autism', 'myths', 'autistic', 'impaired', 'Mottron', 'Montreal', 'IQ', 'scores', 'disturbed', 'repetitive', 'Wisconsin', 'Madison', 'edges', 'toothed', 'determining', 'profound', 'tissue', 'Thus', 'photosynthesis', 'County', 'Ages', 'diagnostic', 'Foot', 'publishes', 'Greeks', 'routinely', 'secretions', 'enzyme', 'diagnosing', 'pigment', 'marker', 'fluid', 'pH', 'IBM', 'optical', 'lithography', 'compact', 'semiconductor', 'variant', 'Meteorological', 'WMO', 'fiction', 'vicious', 'incomplete', 'resembling', 'massage', 'muscles', '].\"', 'depths', 'comprise', 'Sharks', 'IUCN', 'bycatch', 'soup', 'dancers', 'musical', 'predisposition', 'Ebstein', 'courtship', 'societies', 'doctoral', 'variants', 'nerve', 'receptor', 'partially', 'coordination', 'rhythm', 'emphasises', 'motivation', 'inspired', 'quantum', 'physicists', 'algorithms', 'n', 'algorithm', 'inspiration', 'shortest', 'curved', 'hills', 'P1', 'P2', 'Hubble', 'Johns', 'enigmatic', 'eel', 'balmy', 'microscopic', 'infant', 'archipelago', 'radiocarbon', 'survivors', 'Neanderthal', 'signature', 'cores', 'southeastern', 'culturally', 'coincided', 'onset', 'starch', 'RNAi', 'defence', 'amylose', 'bowel', 'enzymes', 'GI', 'enhance', 'ecstasy', 'MDMA', 'harmful', 'Could', 'gamma', 'originated', 'hints', 'cigarette', 'suck', 'PAHs', 'Nobody', 'pollutants', 'waterways', 'accumulate', 'brake', 'Pittsburgh', 'lineage', 'reflux', 'acidity', 'episodes', 'peace', 'fathers', 'primate', 'observed', 'testosterone', 'dads', 'Males', 'monkey', 'foraging', 'string', 'Psychiatry', 'adulthood', 'lifespan', 'versions', 'chromosome', 'UNSW', 'clinically', 'Tambora', 'instantly', 'eruption', 'excavation', 'ash', 'Sigurdsson', '1986', 'sulfur', 'melted', 'charcoal', 'disc', 'newborn', 'mastodon', 'x', 'Epstein', 'syndrome', 'CFS', 'psychiatric', 'concentration', 'scans', 'neutral', 'contraceptive', 'ovarian', 'Breast', 'conceive', 'implants', 'neural', 'gliding', 'Defense', 'Boston', 'penetrate', 'acoustic', 'paralysis', 'portraits', 'Gallery', 'Hammerschmidt', 'Hummel', 'portrait', 'paintings', 'gallery', 'theatre', 'swelling', 'Pitcairn', 'archaeologist', '1990', 'Polynesians', 'Spanish', 'VD17', 'colliding', 'asteroid', 'swiftly', 'categorised', 'encounter', 'Jet', 'briefly', 'Antarctica', 'mammoth', 'IPCC', 'conditioning', 'thermal', 'Hu', 'La', 'Ni', 'ntilde', 'equatorial', 'notorious', 'southwestern', 'lasts', 'persist', 'Oceanic', 'swift', 'prevailing', 'astronaut', 'terrestrial', 'joints', 'spirals', 'engineers', 'swamp', 'Gould', 'radicals', 'electron', 'energetic', 'grids', 'Behnke', 'sunspot', 'span', 'schizophrenia', 'Flanagan', 'passive', 'predictive', 'alien', 'Leopold', 'M', 'uuml', 'ller', 'Vesuvius', '4000', 'compelling', 'dynamic', 'eruptions', 'statistical', 'violent', 'caffeine', 'metabolism', 'Sohemy', 'drank', 'assume', 'Abolhasan', 'microwave', 'networking', 'Buddhism', 'manuscripts', 'bark', 'Allon', 'accelerator', 'artefacts', 'stellar', 'GRB', '050904', 'borne', 'Dust', 'astrophysicist', 'Ruiz', 'Princeton', 'faint', 'primordial', 'helium', 'Snuppy', 'somatic', 'clone', 'fingerprint', 'reconstructed', 'jolt', 'Dolly', 'immature', 'Taleyarkhan', 'fusion', 'labs', 'Purdue', 'Engineering', 'Los', 'Angeles', 'deuterium', 'Engineers', 'altered', 'Britons', 'xenotourism', 'transplants', 'Apice', 'WHO', 'kidney', 'haul', 'DVT', 'cabin', 'airlines', 'clotting', 'V', 'onboard', '1800', '2100', 'array', 'Switzerland', 'pole', 'Enceladus', 'spewing', 'geological', 'geyser', 'crust', 'metabolic', 'Nevertheless', 'Titan', 'goddess', 'gaming', 'Boustead', 'dimensional', 'rodents', 'rodent', 'belongs', 'palaeontologist', 'Tertiary', 'continents', 'Moggridge', 'settlers', 'ants', 'distraction', 'monitors', 'tendency', 'bulge', 'hostile', 'cyclotides', 'insights', 'retained', 'twist', 'potent', 'physically', 'chew', 'graft', 'embedded', 'Siberia', 'encountered', 'Mercury', 'Planetary', 'circulating', 'Bulletin', 'Methane', 'couples', 'advises', 'unexplained', 'Possingham', 'influences', 'databases', 'RFID', 'bats', 'whales', 'Illinois', 'strands', 'assembled', 'helix', 'nano', 'tissues', 'assembly', 'atom', 'scaffold', 'pointed', 'MAOA', 'Schofield', 'catastrophic', 'arsonists', 'expressing', 'hydrothermal', 'patents', 'healing', 'haemoglobin', 'Maldives', 'taller', 'Kench', 'Cleopatra', 'coins', 'depictions', 'wore', 'Kleiner', 'Yale', 'melon', 'cobra', 'worn', 'Parexel', 'antibody', 'TGN1412', 'placebo', 'signalling', 'TeGenero', 'bind', 'CD28', 'CDC', 'clades', 'triggering', 'clade', 'phases', 'os', 'enhanced', 'Von', 'robot', 'nanotubes', 'platinum', 'protons', 'sing', 'songs', 'recordings', 'whistle', 'Male', 'earthquake', 'rubble', 'obscured', 'Einstein', 'scramjet', 'Woomera', 'combustion', 'sucking', 'exhaust', 'combustor', 'elliptical', 'noses', 'roles', 'Arlt', 'sequestration', 'dissolved', 'mixing', 'Greenland', 'comparable', 'melt', 'vibration', 'eclipse', 'textiles', 'antidepressant', 'neurological', 'embryonic', 'ouml', 'illnesses', 'similarities', 'melatonin', 'relaxation', 'REM', 'intrusion', 'conversation', 'Humpback', 'whale', 'humpback', 'Suzuki', 'complexity', 'observers', 'drinkers', 'consist', 'cues', 'hailed', 'observation', 'Oxford', 'shoulders', 'Mathematical', 'Learning', 'axis', 'interface', 'silicon', 'neurones', 'glue', 'neuroscientist', 'Zinn', 'edible', 'compound', 'colony', 'lunar', 'shadow', 'sunset', '2500', 'hiding', 'mantle', 'thinner', 'thicker', 'subtle', 'agile', 'probes', 'Go8', 'newer', 'Stilwell', 'hypothesised', 'yielded', 'compressions', 'CPR', 'avoided', 'lubricant', 'polymers', 'rubbing', 'Larsen', 'Harper', 'isotope', 'meteorites', 'erode', 'Jong', 'Ho', 'replication', 'antiviral', 'implies', 'Cockroaches', 'spiders', 'silent', 'eacute', 'olfactory', 'dish', 'shelters', 'cockroaches', 'optimise', '1930s', 'surrounded', 'L', 'bladders', 'commentator', 'Atala', 'bladder', 'CT', 'computerised', 'Beta', 'arousal', 'lasers', 'emitting', 'discs', 'tsunamis', 'Coral', 'rotation', 'seismic', 'reconstruct', '96', 'regulates', 'brightness', 'Communication', 'Jesus', 'Nof', 'prophylaxis', 'McCaw', '1918', 'Taubenberger', 'comparing', 'Remains', 'Tiktaalik', 'bears', 'limbs', 'vertebrates', 'forearm', 'detectable', 'TGN', '1412', 'Webb', 'Edinburgh', 'printer', 'Rosenberg', 'pendulums', 'counterintuitive', 'equations', 'penguin', 'penguins', 'Pole', 'wing', 'ant', 'Uranus', 'strikingly', 'Mab', 'Keck', 'Geophysical', 'chunks', 'freeloaders', 'tokens', 'spider', 'Kasumovic', 'Andrade', 'redbacks', 'Brady', 'Francisco', 'bat', 'Archives', 'marijuana', 'toilet', 'Oslo', 'Anders', 'retrieval', 'queries', 'ranked', 'cave', 'caves', 'Pleistocene', 'graffiti', 'Guthrie', 'Alaska', 'penis', 'figured', 'spew', 'Finally', 'Usherwood', 'ejaculation', 'Tiefer', 'dysfunction', 'defining', 'Morrow', 'Psychiatric', 'intercourse', 'penetration', 'adhesive', 'sticky', 'Australopithecus', 'ape', 'hominid', 'Lucy', 'Mona', 'poster', 'dots', 'Leonardo', 'Franck', 'Vinci', 'Louvre', 'paint', 'vortex', 'reactor', 'army', 'Psychological', 'pyramid', 'Bosnian', 'pyramids', 'Osmanagic', 'Seb', 'Internal', 'droppings', 'uninfected', 'Personality', 'squid', 'personalities', 'Sinn', 'solitary', 'octopuses', 'shells', 'Web', 'genitalia', 'Baltimore', 'genital', 'Kilgour', 'groom', 'joystick', 'conductive', 'calibrate', 'Imagine', 'functions', 'pictured', 'Mapusaurus', 'Sackett', 'tug', 'fabric', 'optics', 'Michigan', 'Smithsonian', 'collide', 'cosmos', 'relativity', 'collisions', 'supermassive', 'observatory', 'subglacial', 'beads', 'Iceland', 'bipolar', 'Healy', 'Cardiff', 'clinicians', 'DSM', 'saddle', 'vertebrae', 'Cocq', 'battery', 'Pysklywec', 'tectonic', 'subduction', 'caterpillar', '*', 'beetles', 'Emeritus', 'Ladybag', 'keys', 'blinks', 'tennis', 'latitude', 'landing', 'vertical', 'decreases', 'downloaded', 'Hartmann', 'Differences', 'essence', 'Arabic', 'giants', 'Havermann', 'Therefore', 'fermentation', 'Gravity', 'Brunini', 'nanotechnology', 'FOE', 'steering', 'Nanotechnology', 'nanomaterials', 'Discovery', 'redesign', 'foam', 'wedges', 'insulation', 'Hale', 'persuasion', 'adverts', 'neuroscience', 'tension', 'Rempel', 'blogging', 'blogs', 'Fitzgerald', 'mythical', 'nanowires', 'flashes', 'Alamos', 'detectors', 'Array', 'Holwell', 'fertilise', 'tapped', 'oven', 'benzene', 'Vecchi', 'Xinhua', 'Cai', 'Tut', 'circumcised', 'Cruz', 'circumcision', 'zooplankton', 'UFO', 'glow', 'lobes', 'matches', 'pixel', 'correlate', 'dive', 'submerged', 'Musicians', 'Memory', 'Law', 'tempo', 'dolphin', 'whistles', 'individually', 'Sotzing', 'thread', 'precursor', 'fabrics', 'dreaders', 'MRI', 'cue', 'antioxidant', 'wheatgrass', 'chlorophyll', 'tumours', 'stereotypes', 'Sex', 'paternal', 'programmed', 'terrorist', 'drone', 'extinctions', 'uniquely', 'Triton', 'Agnor', 'binary', 'hero', 'kipunji', 'RU486', 'Spitzer', 'meteor', 'occupied', 'graveyards', 'photo', 'artefact', 'Zhang', 'mashup', 'creativity', 'apes', 'Evolutionary', 'associative', 'Data', 'Endeavour', 'Continental', '+', 'garlic', 'palate', 'onion', 'cultivated', 'chimps', 'Toumai', 'chimp', 'chimpanzee', 'Alzheimer', 'hobbit', 'Homo', 'microcephaly', 'Flores', 'sapiens', 'microcephalic', 'HD', '69830', 'worlds', 'wobbles', 'insecticides', 'blowflies', 'collaborators', 'sunscreens', 'TGA', 'nanoparticle', 'nanoscale', 'particle', 'Geologists', 'Physics', 'nuclei', 'ITER', 'sustaining', 'scarecrow', 'jersey', 'emptying', 'Rayner', 'calories', 'Bang', 'continuum', 'threads', 'Louisiana', 'institute', 'Rassoulzadegan', 'Kit', 'replicated', 'brainstem', 'consciousness', 'orientation', 'Hahn', 'Cameroon', 'SIV', 'fluids', 'infects', 'Reichler', 'stratospheric', 'hemispheres', 'latitudes', 'stratosphere', 'invisibility', 'metamaterials', 'microwaves', 'Trek', 'cloak', 'backward', 'Ishii', 'shirt', 'hogwartsia', 'K', 'IGF', 'Conventional', 'OctArm', 'mimic', 'responds', 'happiness', 'microbe', 'laptop', 'assemble', 'dodgem', 'stain', 'keratin', 'secreted', 'ivy', 'Svalbard', 'vault', 'Nimmo', 'blob', 'lava', 'reproduced', 'antennas', 'Revolution', 'burned', 'Bradstock', 'wearable', 'stability', 'Itokawa', 'meteorite', 'Frese', 'Dying', 'Permian', 'figs', 'fig', 'Levine', 'Magellanic', 'Chemistry', 'slices', 'Interestingly', 'formations', 'stromatolites', 'itch', 'Mogil', 'allergies', 'Isisfordia', 'swans', 'Mulder', '149', 'Pictoris', 'hypersensitivity', 'Allergy', 'allergens', 'surgeons', 'cadmium', 'cured', 'immunodeficiency', '1976', 'Puck', 'Division', 'occultation', 'bump', 'cautions', 'cirrhosis', 'nbsp', ';%', 'Licinio', 'Browning', 'whinnies', 'blows', 'vocalisations', 'barks', 'molten', 'contrails', 'ANSTO', 'Gansus', 'maze', 'migraine', 'Houle', 'functioning', 'IWC', 'whaling', 'melanoma', 'toy', 'autonomously', 'permafrost', 'Fowler', 'panda', 'pandas', 'Bruford', 'prosecutors', 'BARD1', 'BRCA1', 'BRCA2', 'profiles', 'degeneration', 'Errico', 'Skhul', 'Oued', 'Djebbana', 'Renne', 'feng', 'shui', 'vaastu', 'gay', 'Bogaert', 'foetuses', 'deprived', 'foetus', 'Palestinian', 'Bateson', 'permeability', 'porous', 'Conture', 'stuttering', 'preschoolers', 'Leakey', 'proboscis', 'Arango', 'arthropods', 'therapeutic', 'CFCs', 'dodo', 'intent', 'terabytes', 'Renugopalakrishnan', 'disk', 'Mileura', 'SKA', 'nozzle', 'flame', 'hamsters', 'prions', 'Silent', 'Hofmann', 'Yang', 'phobia', 'liquids', 'stadium', 'Apgar', 'defects', 'Older', 'Vriesekoop', 'Systems', 'linguistic', 'Rintoul', 'Reith', 'subjected', 'polymerase', 'pups', 'meerkats', 'westerlies', 'wounds', 'dress', 'Collaboration', 'fortis', 'finches', 'Bernstein', 'rum', 'Participants', 'reminiscence', 'Janssen', 'jamais', 'vu', 'Moulin', 'spelling', 'buoys', 'Saxon', 'Saxons', 'Y', 'Ayton', 'resilience', 'Strugnell', 'Wiese', 'Parungao', 'northwest', 'VoIP', 'celebrities', 'bread', 'answering', 'rods', 'wisdom', 'dividing', 'Kear', 'Alternatively', 'Fink', 'nylon', 'Seeman', 'Dandekar', 'milligrams', 'Sogin', 'telomeres', 'lysine', 'LY038', 'Heinemann', 'EBV', 'Maui', 'temple', 'temples', 'Kolb', 'Rieback', 'Archimedes', 'ink', 'Burn', 'idealised', 'Rakic', 'retina', 'hoax', 'Graves', 'snowfall', 'Goodwin', 'Monet', 'PGD', 'BRCA', 'Huggable', 'baleen', 'gentle', 'pygmy', 'blushing', 'Cebis', 'TNT', 'frontal', 'Selway', 'geology', 'IAU', 'tornado', 'Kok', 'Gagliardo', 'photoreceptors', 'PNAS', 'Thorne', 'trilobites', 'Palaeozoic', 'Greenfield', 'Fordham', '*&#', 'Tao', 'Perelman', 'archaic', 'mtDNA', 'jealousy', 'satin', 'bowerbirds', 'Karkanas', 'spindle', 'informal', 'fingertip', 'buttons', 'Hollywood', 'vampires', 'Rapa', 'antisocial', 'aggression', 'Amazonia', 'amber', 'Freeland', 'Mississippi', 'Twilley', 'Ozone', 'Crossley', 'photons', 'caesarean', 'similarity', 'Kotz', 'orexin', 'Kandinsky', 'synaesthesia', 'quark', 'Leinweber', 'Bellwood', 'venom', 'antivenom', 'sulfide', 'grieving', 'Jalland', 'Sudoku', 'Hopfield', 'glaucoma', 'blindness', 'TrES', 'ionosphere', 'Parthenon', 'museums', '134340', 'Archipelago', 'carvings', 'Finlayson', 'Eris', 'Jerrett', 'DDT', 'Malaria', 'Lasker', 'mp3', 'Music', 'flares', 'flare', 'Madagascar', 'stadiums', 'Gratz', 'upright', 'desktop', 'Treder', 'Shilton', 'pavers', 'Beecham', 'automated', 'bogs', 'Steele', 'haze', 'spaniels', 'genres', 'Palestinians', 'NRC', 'silk', 'Kempson', 'Burnham', 'irritants', 'Svenson', 'scraping', 'Berg', 'Ebola', 'YouTube', 'Kornberg', 'transcription', 'sleeves', 'Berzowska', 'Riddell', 'Tensorer', 'Acton', 'kilotonne', 'plutonium', 'Andromeda', 'microengine', 'Dickens', 'Schoffer', 'dull', 'Zeil', 'Srinivasan', 'horns', 'periodic', 'butanol', 'melanin', 'Gogonasus', 'aberrant', 'Drago', 'Phar', 'Lap', 'arsenic', 'synchrotron', 'Philadelphia', 'aacute', 'Opportunity', 'transplantation', 'Cohn', 'zircons', 'Mapes', 'tusks', 'elephants', 'tusk', 'Botai', 'objectifiers', 'Breithaupt', 'aerosols', 'nozzles', 'SIDS', 'Serious', 'jams', 'resveratrol', 'calorie', 'Worm', 'Pegasi', 'SCNT', 'Lahn', 'toddlers', 'rattle', 'Soljacic', 'Helmer', 'bowler', 'Müller', 'M15', 'Woodward', 'Resnick', 'chevrons', 'Rubin', 'Paabo', 'paranormal', 'cottonseed', 'Rathore', 'gossypol', 'handers', 'CNVs', 'NGC', '1316', 'Dubcovsky', 'Godthelp', 'Polonium', 'Litvinenko', 'Priestly', 'gearwheels', 'Holst', 'Pellekaan', 'fragrant', 'Laska', 'Ngarrindjeri', 'reburial', 'Connolly', 'Blakers', 'Zeilinger', 'gorillas', 'Saharan', 'Abu', 'Raddad', 'Marom', 'Mesozoic', 'cervical', 'HPV', 'Clendon', 'Coughlan', 'Fauci', 'cocker', 'nebulae', 'parthenogenesis', 'Daley', 'CME', 'nanowire', 'batfish', 'Merz', 'MS', 'jazz', 'komodo', 'Buley', 'nanobacteria', 'Lieske']\n"
    }
   ],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6) Most Similar words checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('colony', 0.9748450517654419), ('demanding', 0.9737796783447266), ('prevention', 0.973760724067688), ('analyse', 0.9710713624954224), ('odour', 0.9707901477813721), ('via', 0.9701260924339294), ('groundwater', 0.9697530269622803), ('croc', 0.9688011407852173), ('amid', 0.9687286019325256), ('tobacco', 0.9679027795791626)]\n"
    }
   ],
   "source": [
    "similar_words = model.most_similar('thanks')\t\n",
    "print(similar_words)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7) Does not match word from words supplied**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "you\n"
    }
   ],
   "source": [
    "dissimlar_words = model.doesnt_match('See you later, thanks for visiting'.split())\n",
    "print(dissimlar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 8) Finding the similarity between two words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Please provide the similarity between these two words:\n0.907075\n"
    }
   ],
   "source": [
    "similarity_two_words = model.similarity('go','see')\n",
    "print(\"Please provide the similarity between these two words:\")\n",
    "print(similarity_two_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('view', 0.9283434152603149), ('mystery', 0.9185882806777954), ('particular', 0.9108386635780334), ('course', 0.904705286026001), ('nature', 0.8959844708442688), ('position', 0.8925306797027588), ('presence', 0.8919743299484253), ('pattern', 0.8902472257614136), ('possibility', 0.8857266306877136), ('sign', 0.8855419158935547)]\n"
    }
   ],
   "source": [
    "similar = model.similar_by_word('kind')\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "* Word Embedding is a type of word representation that allows words with similar meaning to be understood by machine learning algorithms\n",
    "* Word Embedding is used to compute similar words, Create a group of related words, Feature for text classification, Document clustering, Natural language processing\n",
    "* Word2vec is a shallow two-layered neural network model to produce word embedding for better word representation\n",
    "Word2vec represents words in vector space representation. Words are represented in the form of vectors and placement is done in such a way that similar meaning words appear together and dissimilar words are located far away\n",
    "* Word2vec used 2 architectures Continuous Bag of words (CBOW) and skip gram\n",
    "* CBOW is several times faster than skip gram and provides a better frequency for frequent words whereas skip gram needs a small amount of training data and represents even rare words or phrases.\n",
    "* NLTK and word2vec can be used together create powerful applications\n",
    "* The activation function of the neuron defines the output of that neuron given a set of inputs. In word2vec. Softmax Layer (normalized exponential function) is the output layer function which activates or fires each node. Word2vec also has negative sampling available\n",
    "* Gensim is a topic modeling toolkit which is implemented in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593840964059",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}